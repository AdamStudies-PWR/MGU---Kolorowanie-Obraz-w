{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112d2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
    "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
    "# Some part of the code was referenced from below                              #\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66a1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "color = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa09d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18_color = \"ResNet18_color/\"\n",
    "ResNet18_gray = \"ResNet18_gray/\"\n",
    "PIXELS_DIR = f\"{ResNet18_color}pixel_data/\"\n",
    "PIXELS_DIR_GRAY = f\"{ResNet18_gray}pixel_data_grayscale/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906c3739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fc49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "if color:\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=f'./{PIXELS_DIR}train',\n",
    "                                                 transform=transform)\n",
    "\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root=f'./{PIXELS_DIR}test',\n",
    "                                                transform=transform)\n",
    "else:\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=f'./{PIXELS_DIR_GRAY}train',\n",
    "                                                 transform=transform)\n",
    "\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root=f'./{PIXELS_DIR_GRAY}test',\n",
    "                                                transform=transform)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=128, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b438156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314966a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb75a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd551970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa05186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(epoch):\n",
    "    global curr_lr\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "#         acc = accuracy(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        loss_ = train_loss/(i+1)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss_:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print (f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss_:.4f}, Accuracy: {acc:.4f}\")\n",
    "    logs.write(f\"{epoch+1}, {loss_}, {acc}\")\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a772dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test(epoch):\n",
    "    global best_losses\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = 100. * correct / total\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            loss_ = test_loss/(i+1)\n",
    "\n",
    "        print(f\"Loss: {loss_:.4f}, Accuracy: {acc:.4f}\")\n",
    "        logs.write(f\" {loss_}, {acc}\\n\")\n",
    "\n",
    "    \n",
    "    # Save checkpoint and replace old best model if current model is better\n",
    "    if loss < best_losses:\n",
    "        best_losses = loss\n",
    "        torch.save(model.state_dict(), '{}/model-epoch-{}-losses-{:.3f}.pth'.format(checkpoints,epoch+1,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ed6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = MulticlassAccuracy(num_classes=10).to(device)\n",
    "if color:\n",
    "    checkpoints = f'{ResNet18_color}checkpoints_color'\n",
    "    logs_file = f'{ResNet18_color}logs_color.txt'\n",
    "else:\n",
    "    checkpoints = f'{ResNet18_gray}checkpoints_gray'\n",
    "    logs_file = f'{ResNet18_gray}logs_gray.txt'\n",
    "    \n",
    "os.makedirs(checkpoints, exist_ok=True)\n",
    "best_losses = torch.tensor(1e10)\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "logs = open(logs_file,'w')\n",
    "logs.write(\"Epoch, Loss, Accuracy, Val_loss, Val_accuracy\\n\")\n",
    "logs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d9a6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [100/391], Loss: 1.7985, Accuracy: 32.3984\n",
      "Epoch [1/300], Step [200/391], Loss: 1.6530, Accuracy: 38.3945\n",
      "Epoch [1/300], Step [300/391], Loss: 1.5499, Accuracy: 42.5859\n",
      "Epoch [1/300], Step [391/391], Loss: 1.4777, Accuracy: 45.4240\n",
      "Loss: 1.3529, Accuracy: 50.0200\n",
      "Epoch [2/300], Step [100/391], Loss: 2.2475, Accuracy: 15.9609\n",
      "Epoch [2/300], Step [200/391], Loss: 2.1061, Accuracy: 18.6523\n",
      "Epoch [2/300], Step [300/391], Loss: 2.0406, Accuracy: 20.2865\n",
      "Epoch [2/300], Step [391/391], Loss: 1.9711, Accuracy: 23.2320\n",
      "Loss: 1.6809, Accuracy: 36.9700\n",
      "Epoch [3/300], Step [100/391], Loss: 1.6073, Accuracy: 39.1797\n",
      "Epoch [3/300], Step [200/391], Loss: 1.5247, Accuracy: 42.8906\n",
      "Epoch [3/300], Step [300/391], Loss: 1.4785, Accuracy: 44.9661\n",
      "Epoch [3/300], Step [391/391], Loss: 1.4398, Accuracy: 46.5080\n",
      "Loss: 1.2959, Accuracy: 52.3000\n",
      "Epoch [4/300], Step [100/391], Loss: 1.2851, Accuracy: 53.7500\n",
      "Epoch [4/300], Step [200/391], Loss: 1.2498, Accuracy: 54.8047\n",
      "Epoch [4/300], Step [300/391], Loss: 1.2230, Accuracy: 55.7734\n",
      "Epoch [4/300], Step [391/391], Loss: 1.2045, Accuracy: 56.4120\n",
      "Loss: 1.1076, Accuracy: 59.5500\n",
      "Epoch [5/300], Step [100/391], Loss: 1.1184, Accuracy: 59.5469\n",
      "Epoch [5/300], Step [200/391], Loss: 1.1037, Accuracy: 60.0078\n",
      "Epoch [5/300], Step [300/391], Loss: 1.0890, Accuracy: 60.5833\n",
      "Epoch [5/300], Step [391/391], Loss: 1.0717, Accuracy: 61.2360\n",
      "Loss: 1.0062, Accuracy: 63.7900\n",
      "Epoch [6/300], Step [100/391], Loss: 0.9917, Accuracy: 64.0391\n",
      "Epoch [6/300], Step [200/391], Loss: 0.9929, Accuracy: 64.0000\n",
      "Epoch [6/300], Step [300/391], Loss: 0.9871, Accuracy: 64.2865\n",
      "Epoch [6/300], Step [391/391], Loss: 0.9828, Accuracy: 64.5020\n",
      "Loss: 0.9460, Accuracy: 65.8300\n",
      "Epoch [7/300], Step [100/391], Loss: 0.9479, Accuracy: 66.3438\n",
      "Epoch [7/300], Step [200/391], Loss: 0.9356, Accuracy: 66.4922\n",
      "Epoch [7/300], Step [300/391], Loss: 0.9218, Accuracy: 66.9323\n",
      "Epoch [7/300], Step [391/391], Loss: 0.9146, Accuracy: 67.2340\n",
      "Loss: 0.9256, Accuracy: 66.6200\n",
      "Epoch [8/300], Step [100/391], Loss: 0.8793, Accuracy: 68.9375\n",
      "Epoch [8/300], Step [200/391], Loss: 0.8738, Accuracy: 68.8633\n",
      "Epoch [8/300], Step [300/391], Loss: 0.8626, Accuracy: 69.3021\n",
      "Epoch [8/300], Step [391/391], Loss: 0.8636, Accuracy: 69.2020\n",
      "Loss: 0.8358, Accuracy: 70.0600\n",
      "Epoch [9/300], Step [100/391], Loss: 0.7860, Accuracy: 71.8828\n",
      "Epoch [9/300], Step [200/391], Loss: 0.7870, Accuracy: 71.9375\n",
      "Epoch [9/300], Step [300/391], Loss: 0.7865, Accuracy: 71.8281\n",
      "Epoch [9/300], Step [391/391], Loss: 0.7824, Accuracy: 72.1440\n",
      "Loss: 0.8170, Accuracy: 71.3800\n",
      "Epoch [10/300], Step [100/391], Loss: 0.7460, Accuracy: 73.6641\n",
      "Epoch [10/300], Step [200/391], Loss: 0.7450, Accuracy: 73.8164\n",
      "Epoch [10/300], Step [300/391], Loss: 0.7356, Accuracy: 74.0547\n",
      "Epoch [10/300], Step [391/391], Loss: 0.7360, Accuracy: 74.1220\n",
      "Loss: 0.7591, Accuracy: 73.7000\n",
      "Epoch [11/300], Step [100/391], Loss: 0.7154, Accuracy: 74.8516\n",
      "Epoch [11/300], Step [200/391], Loss: 0.7020, Accuracy: 75.1523\n",
      "Epoch [11/300], Step [300/391], Loss: 0.6981, Accuracy: 75.4427\n",
      "Epoch [11/300], Step [391/391], Loss: 0.6981, Accuracy: 75.4580\n",
      "Loss: 0.7384, Accuracy: 73.9700\n",
      "Epoch [12/300], Step [100/391], Loss: 0.6572, Accuracy: 77.0703\n",
      "Epoch [12/300], Step [200/391], Loss: 0.6536, Accuracy: 77.0703\n",
      "Epoch [12/300], Step [300/391], Loss: 0.6585, Accuracy: 76.8229\n",
      "Epoch [12/300], Step [391/391], Loss: 0.6624, Accuracy: 76.7080\n",
      "Loss: 0.6886, Accuracy: 75.7800\n",
      "Epoch [13/300], Step [100/391], Loss: 0.6272, Accuracy: 78.6562\n",
      "Epoch [13/300], Step [200/391], Loss: 0.6273, Accuracy: 78.3555\n",
      "Epoch [13/300], Step [300/391], Loss: 0.6322, Accuracy: 78.0260\n",
      "Epoch [13/300], Step [391/391], Loss: 0.6311, Accuracy: 78.0400\n",
      "Loss: 0.6718, Accuracy: 77.0700\n",
      "Epoch [14/300], Step [100/391], Loss: 0.5789, Accuracy: 79.6406\n",
      "Epoch [14/300], Step [200/391], Loss: 0.5919, Accuracy: 79.1406\n",
      "Epoch [14/300], Step [300/391], Loss: 0.5976, Accuracy: 79.0833\n",
      "Epoch [14/300], Step [391/391], Loss: 0.6015, Accuracy: 78.8880\n",
      "Loss: 0.6495, Accuracy: 77.4100\n",
      "Epoch [15/300], Step [100/391], Loss: 0.5925, Accuracy: 79.3281\n",
      "Epoch [15/300], Step [200/391], Loss: 0.5895, Accuracy: 79.4609\n",
      "Epoch [15/300], Step [300/391], Loss: 0.5856, Accuracy: 79.6250\n",
      "Epoch [15/300], Step [391/391], Loss: 0.5838, Accuracy: 79.5940\n",
      "Loss: 0.6077, Accuracy: 79.0600\n",
      "Epoch [16/300], Step [100/391], Loss: 0.5480, Accuracy: 80.8359\n",
      "Epoch [16/300], Step [200/391], Loss: 0.5575, Accuracy: 80.7031\n",
      "Epoch [16/300], Step [300/391], Loss: 0.5587, Accuracy: 80.5755\n",
      "Epoch [16/300], Step [391/391], Loss: 0.5626, Accuracy: 80.3560\n",
      "Loss: 0.6310, Accuracy: 78.2800\n",
      "Epoch [17/300], Step [100/391], Loss: 0.5517, Accuracy: 80.4922\n",
      "Epoch [17/300], Step [200/391], Loss: 0.5501, Accuracy: 80.7031\n",
      "Epoch [17/300], Step [300/391], Loss: 0.5502, Accuracy: 80.6979\n",
      "Epoch [17/300], Step [391/391], Loss: 0.5493, Accuracy: 80.7380\n",
      "Loss: 0.6013, Accuracy: 79.2400\n",
      "Epoch [18/300], Step [100/391], Loss: 0.5234, Accuracy: 81.6406\n",
      "Epoch [18/300], Step [200/391], Loss: 0.5306, Accuracy: 81.4648\n",
      "Epoch [18/300], Step [300/391], Loss: 0.5309, Accuracy: 81.5781\n",
      "Epoch [18/300], Step [391/391], Loss: 0.5291, Accuracy: 81.6340\n",
      "Loss: 0.5933, Accuracy: 79.3600\n",
      "Epoch [19/300], Step [100/391], Loss: 0.5173, Accuracy: 81.9375\n",
      "Epoch [19/300], Step [200/391], Loss: 0.5161, Accuracy: 82.0234\n",
      "Epoch [19/300], Step [300/391], Loss: 0.5175, Accuracy: 81.9870\n",
      "Epoch [19/300], Step [391/391], Loss: 0.5148, Accuracy: 82.0560\n",
      "Loss: 0.5894, Accuracy: 80.0700\n",
      "Epoch [20/300], Step [100/391], Loss: 0.5076, Accuracy: 82.3125\n",
      "Epoch [20/300], Step [200/391], Loss: 0.5054, Accuracy: 82.4375\n",
      "Epoch [20/300], Step [300/391], Loss: 0.5018, Accuracy: 82.6042\n",
      "Epoch [20/300], Step [391/391], Loss: 0.5017, Accuracy: 82.5420\n",
      "Loss: 0.5957, Accuracy: 79.7000\n",
      "Epoch [21/300], Step [100/391], Loss: 0.4355, Accuracy: 84.8047\n",
      "Epoch [21/300], Step [200/391], Loss: 0.4326, Accuracy: 84.9102\n",
      "Epoch [21/300], Step [300/391], Loss: 0.4256, Accuracy: 85.0911\n",
      "Epoch [21/300], Step [391/391], Loss: 0.4209, Accuracy: 85.1640\n",
      "Loss: 0.5141, Accuracy: 82.6300\n",
      "Epoch [22/300], Step [100/391], Loss: 0.3960, Accuracy: 86.2891\n",
      "Epoch [22/300], Step [200/391], Loss: 0.3999, Accuracy: 86.2578\n",
      "Epoch [22/300], Step [300/391], Loss: 0.4013, Accuracy: 86.1458\n",
      "Epoch [22/300], Step [391/391], Loss: 0.4035, Accuracy: 86.0180\n",
      "Loss: 0.4965, Accuracy: 82.8000\n",
      "Epoch [23/300], Step [100/391], Loss: 0.3937, Accuracy: 85.8828\n",
      "Epoch [23/300], Step [200/391], Loss: 0.4006, Accuracy: 85.8242\n",
      "Epoch [23/300], Step [300/391], Loss: 0.3952, Accuracy: 86.0182\n",
      "Epoch [23/300], Step [391/391], Loss: 0.3920, Accuracy: 86.1420\n",
      "Loss: 0.5240, Accuracy: 82.2900\n",
      "Epoch [24/300], Step [100/391], Loss: 0.3780, Accuracy: 86.6953\n",
      "Epoch [24/300], Step [200/391], Loss: 0.3746, Accuracy: 86.5898\n",
      "Epoch [24/300], Step [300/391], Loss: 0.3804, Accuracy: 86.5755\n",
      "Epoch [24/300], Step [391/391], Loss: 0.3821, Accuracy: 86.5220\n",
      "Loss: 0.5061, Accuracy: 82.8500\n",
      "Epoch [25/300], Step [100/391], Loss: 0.3738, Accuracy: 86.8438\n",
      "Epoch [25/300], Step [200/391], Loss: 0.3732, Accuracy: 86.7695\n",
      "Epoch [25/300], Step [300/391], Loss: 0.3727, Accuracy: 86.7891\n",
      "Epoch [25/300], Step [391/391], Loss: 0.3744, Accuracy: 86.7380\n",
      "Loss: 0.5291, Accuracy: 82.1700\n",
      "Epoch [26/300], Step [100/391], Loss: 0.3653, Accuracy: 87.3828\n",
      "Epoch [26/300], Step [200/391], Loss: 0.3687, Accuracy: 87.1484\n",
      "Epoch [26/300], Step [300/391], Loss: 0.3691, Accuracy: 87.0312\n",
      "Epoch [26/300], Step [391/391], Loss: 0.3697, Accuracy: 86.9740\n",
      "Loss: 0.4974, Accuracy: 83.3600\n",
      "Epoch [27/300], Step [100/391], Loss: 0.3614, Accuracy: 87.1328\n",
      "Epoch [27/300], Step [200/391], Loss: 0.3619, Accuracy: 87.1680\n",
      "Epoch [27/300], Step [300/391], Loss: 0.3611, Accuracy: 87.2161\n",
      "Epoch [27/300], Step [391/391], Loss: 0.3609, Accuracy: 87.1900\n",
      "Loss: 0.5159, Accuracy: 83.0600\n",
      "Epoch [28/300], Step [100/391], Loss: 0.3541, Accuracy: 87.5391\n",
      "Epoch [28/300], Step [200/391], Loss: 0.3632, Accuracy: 87.1562\n",
      "Epoch [28/300], Step [300/391], Loss: 0.3602, Accuracy: 87.3229\n",
      "Epoch [28/300], Step [391/391], Loss: 0.3613, Accuracy: 87.3280\n",
      "Loss: 0.4972, Accuracy: 83.3000\n",
      "Epoch [29/300], Step [100/391], Loss: 0.3665, Accuracy: 87.0703\n",
      "Epoch [29/300], Step [200/391], Loss: 0.3579, Accuracy: 87.1758\n",
      "Epoch [29/300], Step [300/391], Loss: 0.3527, Accuracy: 87.3958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [391/391], Loss: 0.3516, Accuracy: 87.4560\n",
      "Loss: 0.4871, Accuracy: 83.5600\n",
      "Epoch [30/300], Step [100/391], Loss: 0.3408, Accuracy: 88.0391\n",
      "Epoch [30/300], Step [200/391], Loss: 0.3417, Accuracy: 88.1172\n",
      "Epoch [30/300], Step [300/391], Loss: 0.3454, Accuracy: 87.8073\n",
      "Epoch [30/300], Step [391/391], Loss: 0.3457, Accuracy: 87.8020\n",
      "Loss: 0.4975, Accuracy: 82.7600\n",
      "Epoch [31/300], Step [100/391], Loss: 0.3497, Accuracy: 87.8984\n",
      "Epoch [31/300], Step [200/391], Loss: 0.3402, Accuracy: 88.0820\n",
      "Epoch [31/300], Step [300/391], Loss: 0.3409, Accuracy: 88.0000\n",
      "Epoch [31/300], Step [391/391], Loss: 0.3436, Accuracy: 87.8320\n",
      "Loss: 0.5071, Accuracy: 83.1400\n",
      "Epoch [32/300], Step [100/391], Loss: 0.3174, Accuracy: 88.5312\n",
      "Epoch [32/300], Step [200/391], Loss: 0.3199, Accuracy: 88.4375\n",
      "Epoch [32/300], Step [300/391], Loss: 0.3308, Accuracy: 88.2474\n",
      "Epoch [32/300], Step [391/391], Loss: 0.3339, Accuracy: 88.1040\n",
      "Loss: 0.4900, Accuracy: 83.5900\n",
      "Epoch [33/300], Step [100/391], Loss: 0.3360, Accuracy: 87.8984\n",
      "Epoch [33/300], Step [200/391], Loss: 0.3243, Accuracy: 88.3789\n",
      "Epoch [33/300], Step [300/391], Loss: 0.3260, Accuracy: 88.3542\n",
      "Epoch [33/300], Step [391/391], Loss: 0.3293, Accuracy: 88.2800\n",
      "Loss: 0.4747, Accuracy: 84.0500\n",
      "Epoch [34/300], Step [100/391], Loss: 0.3075, Accuracy: 89.2031\n",
      "Epoch [34/300], Step [200/391], Loss: 0.3186, Accuracy: 88.7773\n",
      "Epoch [34/300], Step [300/391], Loss: 0.3208, Accuracy: 88.5938\n",
      "Epoch [34/300], Step [391/391], Loss: 0.3238, Accuracy: 88.5180\n",
      "Loss: 0.5104, Accuracy: 83.5900\n",
      "Epoch [35/300], Step [100/391], Loss: 0.3163, Accuracy: 88.7422\n",
      "Epoch [35/300], Step [200/391], Loss: 0.3218, Accuracy: 88.4883\n",
      "Epoch [35/300], Step [300/391], Loss: 0.3228, Accuracy: 88.5990\n",
      "Epoch [35/300], Step [391/391], Loss: 0.3238, Accuracy: 88.5900\n",
      "Loss: 0.4891, Accuracy: 83.9700\n",
      "Epoch [36/300], Step [100/391], Loss: 0.3065, Accuracy: 89.2734\n",
      "Epoch [36/300], Step [200/391], Loss: 0.3098, Accuracy: 89.0898\n",
      "Epoch [36/300], Step [300/391], Loss: 0.3118, Accuracy: 88.9271\n",
      "Epoch [36/300], Step [391/391], Loss: 0.3167, Accuracy: 88.7160\n",
      "Loss: 0.5030, Accuracy: 83.4800\n",
      "Epoch [37/300], Step [100/391], Loss: 0.3030, Accuracy: 89.2812\n",
      "Epoch [37/300], Step [200/391], Loss: 0.3127, Accuracy: 88.9023\n",
      "Epoch [37/300], Step [300/391], Loss: 0.3122, Accuracy: 88.9245\n",
      "Epoch [37/300], Step [391/391], Loss: 0.3125, Accuracy: 88.9560\n",
      "Loss: 0.4736, Accuracy: 84.4200\n",
      "Epoch [38/300], Step [100/391], Loss: 0.3023, Accuracy: 89.2891\n",
      "Epoch [38/300], Step [200/391], Loss: 0.3068, Accuracy: 89.2812\n",
      "Epoch [38/300], Step [300/391], Loss: 0.3076, Accuracy: 89.1328\n",
      "Epoch [38/300], Step [391/391], Loss: 0.3072, Accuracy: 89.0640\n",
      "Loss: 0.4701, Accuracy: 84.5700\n",
      "Epoch [39/300], Step [100/391], Loss: 0.2812, Accuracy: 89.8281\n",
      "Epoch [39/300], Step [200/391], Loss: 0.2936, Accuracy: 89.5195\n",
      "Epoch [39/300], Step [300/391], Loss: 0.2972, Accuracy: 89.5026\n",
      "Epoch [39/300], Step [391/391], Loss: 0.3028, Accuracy: 89.3620\n",
      "Loss: 0.4796, Accuracy: 84.2200\n",
      "Epoch [40/300], Step [100/391], Loss: 0.2959, Accuracy: 89.4844\n",
      "Epoch [40/300], Step [200/391], Loss: 0.2984, Accuracy: 89.3242\n",
      "Epoch [40/300], Step [300/391], Loss: 0.2990, Accuracy: 89.3125\n",
      "Epoch [40/300], Step [391/391], Loss: 0.2996, Accuracy: 89.3080\n",
      "Loss: 0.4813, Accuracy: 84.4700\n",
      "Epoch [41/300], Step [100/391], Loss: 0.2731, Accuracy: 90.4297\n",
      "Epoch [41/300], Step [200/391], Loss: 0.2651, Accuracy: 90.6953\n",
      "Epoch [41/300], Step [300/391], Loss: 0.2628, Accuracy: 90.7526\n",
      "Epoch [41/300], Step [391/391], Loss: 0.2613, Accuracy: 90.7380\n",
      "Loss: 0.4852, Accuracy: 84.5600\n",
      "Epoch [42/300], Step [100/391], Loss: 0.2515, Accuracy: 91.1641\n",
      "Epoch [42/300], Step [200/391], Loss: 0.2558, Accuracy: 90.8281\n",
      "Epoch [42/300], Step [300/391], Loss: 0.2566, Accuracy: 90.8125\n",
      "Epoch [42/300], Step [391/391], Loss: 0.2575, Accuracy: 90.7960\n",
      "Loss: 0.4673, Accuracy: 85.1600\n",
      "Epoch [43/300], Step [100/391], Loss: 0.2517, Accuracy: 91.2500\n",
      "Epoch [43/300], Step [200/391], Loss: 0.2480, Accuracy: 91.3633\n",
      "Epoch [43/300], Step [300/391], Loss: 0.2499, Accuracy: 91.2292\n",
      "Epoch [43/300], Step [391/391], Loss: 0.2517, Accuracy: 91.1580\n",
      "Loss: 0.4677, Accuracy: 85.2600\n",
      "Epoch [44/300], Step [100/391], Loss: 0.2405, Accuracy: 91.5156\n",
      "Epoch [44/300], Step [200/391], Loss: 0.2487, Accuracy: 91.1445\n",
      "Epoch [44/300], Step [300/391], Loss: 0.2509, Accuracy: 91.0807\n",
      "Epoch [44/300], Step [391/391], Loss: 0.2495, Accuracy: 91.1040\n",
      "Loss: 0.4815, Accuracy: 84.4800\n",
      "Epoch [45/300], Step [100/391], Loss: 0.2462, Accuracy: 91.0938\n",
      "Epoch [45/300], Step [200/391], Loss: 0.2483, Accuracy: 91.0156\n",
      "Epoch [45/300], Step [300/391], Loss: 0.2490, Accuracy: 91.1224\n",
      "Epoch [45/300], Step [391/391], Loss: 0.2503, Accuracy: 91.0920\n",
      "Loss: 0.4751, Accuracy: 84.7100\n",
      "Epoch [46/300], Step [100/391], Loss: 0.2352, Accuracy: 91.6875\n",
      "Epoch [46/300], Step [200/391], Loss: 0.2383, Accuracy: 91.6562\n",
      "Epoch [46/300], Step [300/391], Loss: 0.2462, Accuracy: 91.3854\n",
      "Epoch [46/300], Step [391/391], Loss: 0.2481, Accuracy: 91.3040\n",
      "Loss: 0.4722, Accuracy: 84.9900\n",
      "Epoch [47/300], Step [100/391], Loss: 0.2520, Accuracy: 91.1016\n",
      "Epoch [47/300], Step [200/391], Loss: 0.2482, Accuracy: 91.1367\n",
      "Epoch [47/300], Step [300/391], Loss: 0.2457, Accuracy: 91.2344\n",
      "Epoch [47/300], Step [391/391], Loss: 0.2434, Accuracy: 91.3300\n",
      "Loss: 0.4659, Accuracy: 84.8500\n",
      "Epoch [48/300], Step [100/391], Loss: 0.2449, Accuracy: 91.3984\n",
      "Epoch [48/300], Step [200/391], Loss: 0.2425, Accuracy: 91.5430\n",
      "Epoch [48/300], Step [300/391], Loss: 0.2415, Accuracy: 91.4271\n",
      "Epoch [48/300], Step [391/391], Loss: 0.2425, Accuracy: 91.4020\n",
      "Loss: 0.4810, Accuracy: 84.8900\n",
      "Epoch [49/300], Step [100/391], Loss: 0.2400, Accuracy: 91.6250\n",
      "Epoch [49/300], Step [200/391], Loss: 0.2380, Accuracy: 91.5586\n",
      "Epoch [49/300], Step [300/391], Loss: 0.2406, Accuracy: 91.4271\n",
      "Epoch [49/300], Step [391/391], Loss: 0.2407, Accuracy: 91.3580\n",
      "Loss: 0.4792, Accuracy: 84.6600\n",
      "Epoch [50/300], Step [100/391], Loss: 0.2348, Accuracy: 91.4297\n",
      "Epoch [50/300], Step [200/391], Loss: 0.2360, Accuracy: 91.5586\n",
      "Epoch [50/300], Step [300/391], Loss: 0.2356, Accuracy: 91.4922\n",
      "Epoch [50/300], Step [391/391], Loss: 0.2375, Accuracy: 91.4980\n",
      "Loss: 0.4915, Accuracy: 84.3900\n",
      "Epoch [51/300], Step [100/391], Loss: 0.2444, Accuracy: 91.1797\n",
      "Epoch [51/300], Step [200/391], Loss: 0.2356, Accuracy: 91.5156\n",
      "Epoch [51/300], Step [300/391], Loss: 0.2374, Accuracy: 91.5078\n",
      "Epoch [51/300], Step [391/391], Loss: 0.2385, Accuracy: 91.4860\n",
      "Loss: 0.4798, Accuracy: 85.5000\n",
      "Epoch [52/300], Step [100/391], Loss: 0.2401, Accuracy: 91.3750\n",
      "Epoch [52/300], Step [200/391], Loss: 0.2339, Accuracy: 91.5508\n",
      "Epoch [52/300], Step [300/391], Loss: 0.2369, Accuracy: 91.4766\n",
      "Epoch [52/300], Step [391/391], Loss: 0.2361, Accuracy: 91.5520\n",
      "Loss: 0.4955, Accuracy: 84.8400\n",
      "Epoch [53/300], Step [100/391], Loss: 0.2241, Accuracy: 91.8984\n",
      "Epoch [53/300], Step [200/391], Loss: 0.2286, Accuracy: 91.8477\n",
      "Epoch [53/300], Step [300/391], Loss: 0.2314, Accuracy: 91.7057\n",
      "Epoch [53/300], Step [391/391], Loss: 0.2312, Accuracy: 91.7140\n",
      "Loss: 0.4803, Accuracy: 85.0900\n",
      "Epoch [54/300], Step [100/391], Loss: 0.2233, Accuracy: 92.1328\n",
      "Epoch [54/300], Step [200/391], Loss: 0.2258, Accuracy: 92.0586\n",
      "Epoch [54/300], Step [300/391], Loss: 0.2296, Accuracy: 91.8438\n",
      "Epoch [54/300], Step [391/391], Loss: 0.2308, Accuracy: 91.7920\n",
      "Loss: 0.4915, Accuracy: 84.5900\n",
      "Epoch [55/300], Step [100/391], Loss: 0.2241, Accuracy: 92.0312\n",
      "Epoch [55/300], Step [200/391], Loss: 0.2310, Accuracy: 91.7617\n",
      "Epoch [55/300], Step [300/391], Loss: 0.2296, Accuracy: 91.8125\n",
      "Epoch [55/300], Step [391/391], Loss: 0.2304, Accuracy: 91.7380\n",
      "Loss: 0.4944, Accuracy: 84.4900\n",
      "Epoch [56/300], Step [100/391], Loss: 0.2148, Accuracy: 92.5312\n",
      "Epoch [56/300], Step [200/391], Loss: 0.2206, Accuracy: 92.3438\n",
      "Epoch [56/300], Step [300/391], Loss: 0.2243, Accuracy: 92.1823\n",
      "Epoch [56/300], Step [391/391], Loss: 0.2247, Accuracy: 92.1360\n",
      "Loss: 0.4768, Accuracy: 84.9200\n",
      "Epoch [57/300], Step [100/391], Loss: 0.2300, Accuracy: 91.8047\n",
      "Epoch [57/300], Step [200/391], Loss: 0.2267, Accuracy: 91.9531\n",
      "Epoch [57/300], Step [300/391], Loss: 0.2255, Accuracy: 92.0260\n",
      "Epoch [57/300], Step [391/391], Loss: 0.2265, Accuracy: 92.0040\n",
      "Loss: 0.4863, Accuracy: 84.9000\n",
      "Epoch [58/300], Step [100/391], Loss: 0.2259, Accuracy: 92.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Step [200/391], Loss: 0.2227, Accuracy: 92.1758\n",
      "Epoch [58/300], Step [300/391], Loss: 0.2253, Accuracy: 91.9974\n",
      "Epoch [58/300], Step [391/391], Loss: 0.2250, Accuracy: 92.0440\n",
      "Loss: 0.4913, Accuracy: 85.1500\n",
      "Epoch [59/300], Step [100/391], Loss: 0.2180, Accuracy: 92.1250\n",
      "Epoch [59/300], Step [200/391], Loss: 0.2258, Accuracy: 91.9688\n",
      "Epoch [59/300], Step [300/391], Loss: 0.2232, Accuracy: 92.1042\n",
      "Epoch [59/300], Step [391/391], Loss: 0.2221, Accuracy: 92.1500\n",
      "Loss: 0.4979, Accuracy: 84.8500\n",
      "Epoch [60/300], Step [100/391], Loss: 0.2226, Accuracy: 92.2656\n",
      "Epoch [60/300], Step [200/391], Loss: 0.2224, Accuracy: 92.0938\n",
      "Epoch [60/300], Step [300/391], Loss: 0.2230, Accuracy: 92.0312\n",
      "Epoch [60/300], Step [391/391], Loss: 0.2225, Accuracy: 92.0160\n",
      "Loss: 0.5020, Accuracy: 84.8800\n",
      "Epoch [61/300], Step [100/391], Loss: 0.2026, Accuracy: 92.9141\n",
      "Epoch [61/300], Step [200/391], Loss: 0.2067, Accuracy: 92.6992\n",
      "Epoch [61/300], Step [300/391], Loss: 0.2050, Accuracy: 92.7422\n",
      "Epoch [61/300], Step [391/391], Loss: 0.2049, Accuracy: 92.7180\n",
      "Loss: 0.4898, Accuracy: 85.3900\n",
      "Epoch [62/300], Step [100/391], Loss: 0.1944, Accuracy: 93.2578\n",
      "Epoch [62/300], Step [200/391], Loss: 0.1944, Accuracy: 93.1641\n",
      "Epoch [62/300], Step [300/391], Loss: 0.1994, Accuracy: 92.8984\n",
      "Epoch [62/300], Step [391/391], Loss: 0.2021, Accuracy: 92.8080\n",
      "Loss: 0.4862, Accuracy: 85.3700\n",
      "Epoch [63/300], Step [100/391], Loss: 0.2127, Accuracy: 92.5938\n",
      "Epoch [63/300], Step [200/391], Loss: 0.2083, Accuracy: 92.6367\n",
      "Epoch [63/300], Step [300/391], Loss: 0.2078, Accuracy: 92.7552\n",
      "Epoch [63/300], Step [391/391], Loss: 0.2060, Accuracy: 92.7600\n",
      "Loss: 0.5004, Accuracy: 85.3600\n",
      "Epoch [64/300], Step [100/391], Loss: 0.2055, Accuracy: 92.7188\n",
      "Epoch [64/300], Step [200/391], Loss: 0.2040, Accuracy: 92.5703\n",
      "Epoch [64/300], Step [300/391], Loss: 0.2024, Accuracy: 92.6016\n",
      "Epoch [64/300], Step [391/391], Loss: 0.2041, Accuracy: 92.6300\n",
      "Loss: 0.4870, Accuracy: 85.2700\n",
      "Epoch [65/300], Step [100/391], Loss: 0.2050, Accuracy: 92.8125\n",
      "Epoch [65/300], Step [200/391], Loss: 0.2037, Accuracy: 92.7891\n",
      "Epoch [65/300], Step [300/391], Loss: 0.2039, Accuracy: 92.7422\n",
      "Epoch [65/300], Step [391/391], Loss: 0.2041, Accuracy: 92.7380\n",
      "Loss: 0.4847, Accuracy: 85.7500\n",
      "Epoch [66/300], Step [100/391], Loss: 0.2042, Accuracy: 92.7969\n",
      "Epoch [66/300], Step [200/391], Loss: 0.2018, Accuracy: 92.9453\n",
      "Epoch [66/300], Step [300/391], Loss: 0.2032, Accuracy: 92.7917\n",
      "Epoch [66/300], Step [391/391], Loss: 0.2026, Accuracy: 92.8080\n",
      "Loss: 0.4861, Accuracy: 85.5500\n",
      "Epoch [67/300], Step [100/391], Loss: 0.2024, Accuracy: 92.4688\n",
      "Epoch [67/300], Step [200/391], Loss: 0.1994, Accuracy: 92.7188\n",
      "Epoch [67/300], Step [300/391], Loss: 0.1994, Accuracy: 92.7708\n",
      "Epoch [67/300], Step [391/391], Loss: 0.2004, Accuracy: 92.8120\n",
      "Loss: 0.4879, Accuracy: 85.4800\n",
      "Epoch [68/300], Step [100/391], Loss: 0.2011, Accuracy: 92.7578\n",
      "Epoch [68/300], Step [200/391], Loss: 0.2032, Accuracy: 92.7734\n",
      "Epoch [68/300], Step [300/391], Loss: 0.2027, Accuracy: 92.8203\n",
      "Epoch [68/300], Step [391/391], Loss: 0.2016, Accuracy: 92.8440\n",
      "Loss: 0.4787, Accuracy: 85.3800\n",
      "Epoch [69/300], Step [100/391], Loss: 0.1995, Accuracy: 92.6406\n",
      "Epoch [69/300], Step [200/391], Loss: 0.1993, Accuracy: 92.7109\n",
      "Epoch [69/300], Step [300/391], Loss: 0.2002, Accuracy: 92.7760\n",
      "Epoch [69/300], Step [391/391], Loss: 0.1997, Accuracy: 92.7960\n",
      "Loss: 0.4774, Accuracy: 85.6000\n",
      "Epoch [70/300], Step [100/391], Loss: 0.1920, Accuracy: 93.2969\n",
      "Epoch [70/300], Step [200/391], Loss: 0.1986, Accuracy: 92.9336\n",
      "Epoch [70/300], Step [300/391], Loss: 0.2007, Accuracy: 92.8880\n",
      "Epoch [70/300], Step [391/391], Loss: 0.2017, Accuracy: 92.7940\n",
      "Loss: 0.4851, Accuracy: 85.1800\n",
      "Epoch [71/300], Step [100/391], Loss: 0.1911, Accuracy: 93.3047\n",
      "Epoch [71/300], Step [200/391], Loss: 0.1948, Accuracy: 93.1797\n",
      "Epoch [71/300], Step [300/391], Loss: 0.1984, Accuracy: 93.0339\n",
      "Epoch [71/300], Step [391/391], Loss: 0.1984, Accuracy: 93.0200\n",
      "Loss: 0.4913, Accuracy: 85.2800\n",
      "Epoch [72/300], Step [100/391], Loss: 0.1968, Accuracy: 92.8438\n",
      "Epoch [72/300], Step [200/391], Loss: 0.1979, Accuracy: 92.8906\n",
      "Epoch [72/300], Step [300/391], Loss: 0.2012, Accuracy: 92.7656\n",
      "Epoch [72/300], Step [391/391], Loss: 0.1997, Accuracy: 92.8800\n",
      "Loss: 0.4966, Accuracy: 85.2500\n",
      "Epoch [73/300], Step [100/391], Loss: 0.1893, Accuracy: 93.2266\n",
      "Epoch [73/300], Step [200/391], Loss: 0.1912, Accuracy: 93.1992\n",
      "Epoch [73/300], Step [300/391], Loss: 0.1950, Accuracy: 93.1250\n",
      "Epoch [73/300], Step [391/391], Loss: 0.1969, Accuracy: 92.9660\n",
      "Loss: 0.4904, Accuracy: 85.4800\n",
      "Epoch [74/300], Step [100/391], Loss: 0.1926, Accuracy: 93.0781\n",
      "Epoch [74/300], Step [200/391], Loss: 0.1988, Accuracy: 92.8750\n",
      "Epoch [74/300], Step [300/391], Loss: 0.1968, Accuracy: 92.9089\n",
      "Epoch [74/300], Step [391/391], Loss: 0.1973, Accuracy: 92.9420\n",
      "Loss: 0.4985, Accuracy: 85.3800\n",
      "Epoch [75/300], Step [100/391], Loss: 0.1931, Accuracy: 93.2500\n",
      "Epoch [75/300], Step [200/391], Loss: 0.1935, Accuracy: 93.2188\n",
      "Epoch [75/300], Step [300/391], Loss: 0.1965, Accuracy: 93.0443\n",
      "Epoch [75/300], Step [391/391], Loss: 0.1963, Accuracy: 93.0000\n",
      "Loss: 0.5022, Accuracy: 85.3300\n",
      "Epoch [76/300], Step [100/391], Loss: 0.1943, Accuracy: 93.2266\n",
      "Epoch [76/300], Step [200/391], Loss: 0.1908, Accuracy: 93.1719\n",
      "Epoch [76/300], Step [300/391], Loss: 0.1912, Accuracy: 93.1901\n",
      "Epoch [76/300], Step [391/391], Loss: 0.1914, Accuracy: 93.1680\n",
      "Loss: 0.5139, Accuracy: 84.8600\n",
      "Epoch [77/300], Step [100/391], Loss: 0.1952, Accuracy: 92.8594\n",
      "Epoch [77/300], Step [200/391], Loss: 0.1920, Accuracy: 93.2500\n",
      "Epoch [77/300], Step [300/391], Loss: 0.1956, Accuracy: 93.1198\n",
      "Epoch [77/300], Step [391/391], Loss: 0.1958, Accuracy: 93.1200\n",
      "Loss: 0.5041, Accuracy: 85.3600\n",
      "Epoch [78/300], Step [100/391], Loss: 0.1924, Accuracy: 93.2578\n",
      "Epoch [78/300], Step [200/391], Loss: 0.1947, Accuracy: 93.0898\n",
      "Epoch [78/300], Step [300/391], Loss: 0.1931, Accuracy: 93.0990\n",
      "Epoch [78/300], Step [391/391], Loss: 0.1919, Accuracy: 93.1140\n",
      "Loss: 0.4924, Accuracy: 85.3100\n",
      "Epoch [79/300], Step [100/391], Loss: 0.1980, Accuracy: 92.7578\n",
      "Epoch [79/300], Step [200/391], Loss: 0.1972, Accuracy: 92.8203\n",
      "Epoch [79/300], Step [300/391], Loss: 0.1959, Accuracy: 92.9349\n",
      "Epoch [79/300], Step [391/391], Loss: 0.1944, Accuracy: 92.9940\n",
      "Loss: 0.5012, Accuracy: 85.0300\n",
      "Epoch [80/300], Step [100/391], Loss: 0.2073, Accuracy: 92.5938\n",
      "Epoch [80/300], Step [200/391], Loss: 0.2032, Accuracy: 92.7344\n",
      "Epoch [80/300], Step [300/391], Loss: 0.1979, Accuracy: 92.9427\n",
      "Epoch [80/300], Step [391/391], Loss: 0.1936, Accuracy: 93.0380\n",
      "Loss: 0.4992, Accuracy: 85.3500\n",
      "Epoch [81/300], Step [100/391], Loss: 0.1888, Accuracy: 93.0156\n",
      "Epoch [81/300], Step [200/391], Loss: 0.1865, Accuracy: 93.1836\n",
      "Epoch [81/300], Step [300/391], Loss: 0.1872, Accuracy: 93.2474\n",
      "Epoch [81/300], Step [391/391], Loss: 0.1882, Accuracy: 93.2140\n",
      "Loss: 0.4964, Accuracy: 85.4400\n",
      "Epoch [82/300], Step [100/391], Loss: 0.1876, Accuracy: 93.5312\n",
      "Epoch [82/300], Step [200/391], Loss: 0.1907, Accuracy: 93.3164\n",
      "Epoch [82/300], Step [300/391], Loss: 0.1909, Accuracy: 93.3125\n",
      "Epoch [82/300], Step [391/391], Loss: 0.1894, Accuracy: 93.3520\n",
      "Loss: 0.5003, Accuracy: 85.0200\n",
      "Epoch [83/300], Step [100/391], Loss: 0.1850, Accuracy: 93.5703\n",
      "Epoch [83/300], Step [200/391], Loss: 0.1840, Accuracy: 93.5195\n",
      "Epoch [83/300], Step [300/391], Loss: 0.1828, Accuracy: 93.4896\n",
      "Epoch [83/300], Step [391/391], Loss: 0.1839, Accuracy: 93.4180\n",
      "Loss: 0.5099, Accuracy: 85.3400\n",
      "Epoch [84/300], Step [100/391], Loss: 0.1866, Accuracy: 93.0078\n",
      "Epoch [84/300], Step [200/391], Loss: 0.1863, Accuracy: 93.2617\n",
      "Epoch [84/300], Step [300/391], Loss: 0.1871, Accuracy: 93.2474\n",
      "Epoch [84/300], Step [391/391], Loss: 0.1865, Accuracy: 93.3160\n",
      "Loss: 0.4982, Accuracy: 85.1300\n",
      "Epoch [85/300], Step [100/391], Loss: 0.1922, Accuracy: 93.0547\n",
      "Epoch [85/300], Step [200/391], Loss: 0.1898, Accuracy: 93.3477\n",
      "Epoch [85/300], Step [300/391], Loss: 0.1869, Accuracy: 93.4219\n",
      "Epoch [85/300], Step [391/391], Loss: 0.1850, Accuracy: 93.3980\n",
      "Loss: 0.5020, Accuracy: 84.9600\n",
      "Epoch [86/300], Step [100/391], Loss: 0.1929, Accuracy: 93.0781\n",
      "Epoch [86/300], Step [200/391], Loss: 0.1866, Accuracy: 93.3203\n",
      "Epoch [86/300], Step [300/391], Loss: 0.1856, Accuracy: 93.3229\n",
      "Epoch [86/300], Step [391/391], Loss: 0.1872, Accuracy: 93.2440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4931, Accuracy: 85.7000\n",
      "Epoch [87/300], Step [100/391], Loss: 0.1885, Accuracy: 93.1641\n",
      "Epoch [87/300], Step [200/391], Loss: 0.1835, Accuracy: 93.4297\n",
      "Epoch [87/300], Step [300/391], Loss: 0.1871, Accuracy: 93.3464\n",
      "Epoch [87/300], Step [391/391], Loss: 0.1867, Accuracy: 93.3400\n",
      "Loss: 0.5034, Accuracy: 85.4300\n",
      "Epoch [88/300], Step [100/391], Loss: 0.1818, Accuracy: 93.5078\n",
      "Epoch [88/300], Step [200/391], Loss: 0.1826, Accuracy: 93.4727\n",
      "Epoch [88/300], Step [300/391], Loss: 0.1826, Accuracy: 93.4427\n",
      "Epoch [88/300], Step [391/391], Loss: 0.1841, Accuracy: 93.3740\n",
      "Loss: 0.5035, Accuracy: 85.2800\n",
      "Epoch [89/300], Step [100/391], Loss: 0.1839, Accuracy: 93.4219\n",
      "Epoch [89/300], Step [200/391], Loss: 0.1866, Accuracy: 93.4414\n",
      "Epoch [89/300], Step [300/391], Loss: 0.1841, Accuracy: 93.4688\n",
      "Epoch [89/300], Step [391/391], Loss: 0.1861, Accuracy: 93.4100\n",
      "Loss: 0.5212, Accuracy: 84.9700\n",
      "Epoch [90/300], Step [100/391], Loss: 0.1845, Accuracy: 93.5469\n",
      "Epoch [90/300], Step [200/391], Loss: 0.1836, Accuracy: 93.5195\n",
      "Epoch [90/300], Step [300/391], Loss: 0.1852, Accuracy: 93.4010\n",
      "Epoch [90/300], Step [391/391], Loss: 0.1857, Accuracy: 93.3740\n",
      "Loss: 0.4969, Accuracy: 85.2700\n",
      "Epoch [91/300], Step [100/391], Loss: 0.1897, Accuracy: 93.1484\n",
      "Epoch [91/300], Step [200/391], Loss: 0.1870, Accuracy: 93.3125\n",
      "Epoch [91/300], Step [300/391], Loss: 0.1893, Accuracy: 93.2786\n",
      "Epoch [91/300], Step [391/391], Loss: 0.1883, Accuracy: 93.3100\n",
      "Loss: 0.5022, Accuracy: 85.4300\n",
      "Epoch [92/300], Step [100/391], Loss: 0.1791, Accuracy: 93.6250\n",
      "Epoch [92/300], Step [200/391], Loss: 0.1853, Accuracy: 93.5273\n",
      "Epoch [92/300], Step [300/391], Loss: 0.1843, Accuracy: 93.5130\n",
      "Epoch [92/300], Step [391/391], Loss: 0.1838, Accuracy: 93.5520\n",
      "Loss: 0.4931, Accuracy: 85.4800\n",
      "Epoch [93/300], Step [100/391], Loss: 0.1825, Accuracy: 93.2344\n",
      "Epoch [93/300], Step [200/391], Loss: 0.1804, Accuracy: 93.3555\n",
      "Epoch [93/300], Step [300/391], Loss: 0.1812, Accuracy: 93.4219\n",
      "Epoch [93/300], Step [391/391], Loss: 0.1816, Accuracy: 93.4500\n",
      "Loss: 0.5072, Accuracy: 85.0400\n",
      "Epoch [94/300], Step [100/391], Loss: 0.1841, Accuracy: 93.0859\n",
      "Epoch [94/300], Step [200/391], Loss: 0.1818, Accuracy: 93.2812\n",
      "Epoch [94/300], Step [300/391], Loss: 0.1850, Accuracy: 93.2292\n",
      "Epoch [94/300], Step [391/391], Loss: 0.1843, Accuracy: 93.3440\n",
      "Loss: 0.5098, Accuracy: 84.9700\n",
      "Epoch [95/300], Step [100/391], Loss: 0.1892, Accuracy: 93.3516\n",
      "Epoch [95/300], Step [200/391], Loss: 0.1839, Accuracy: 93.3672\n",
      "Epoch [95/300], Step [300/391], Loss: 0.1843, Accuracy: 93.4062\n",
      "Epoch [95/300], Step [391/391], Loss: 0.1852, Accuracy: 93.3800\n",
      "Loss: 0.4979, Accuracy: 85.7100\n",
      "Epoch [96/300], Step [100/391], Loss: 0.1826, Accuracy: 93.3750\n",
      "Epoch [96/300], Step [200/391], Loss: 0.1840, Accuracy: 93.3828\n",
      "Epoch [96/300], Step [300/391], Loss: 0.1831, Accuracy: 93.3724\n",
      "Epoch [96/300], Step [391/391], Loss: 0.1823, Accuracy: 93.4800\n",
      "Loss: 0.5109, Accuracy: 85.0200\n",
      "Epoch [97/300], Step [100/391], Loss: 0.1836, Accuracy: 93.4844\n",
      "Epoch [97/300], Step [200/391], Loss: 0.1807, Accuracy: 93.5977\n",
      "Epoch [97/300], Step [300/391], Loss: 0.1831, Accuracy: 93.5078\n",
      "Epoch [97/300], Step [391/391], Loss: 0.1816, Accuracy: 93.5220\n",
      "Loss: 0.5009, Accuracy: 85.2400\n",
      "Epoch [98/300], Step [100/391], Loss: 0.1788, Accuracy: 93.6094\n",
      "Epoch [98/300], Step [200/391], Loss: 0.1810, Accuracy: 93.5430\n",
      "Epoch [98/300], Step [300/391], Loss: 0.1842, Accuracy: 93.4479\n",
      "Epoch [98/300], Step [391/391], Loss: 0.1835, Accuracy: 93.4940\n",
      "Loss: 0.4986, Accuracy: 85.0700\n",
      "Epoch [99/300], Step [100/391], Loss: 0.1902, Accuracy: 93.0938\n",
      "Epoch [99/300], Step [200/391], Loss: 0.1849, Accuracy: 93.4141\n",
      "Epoch [99/300], Step [300/391], Loss: 0.1834, Accuracy: 93.5573\n",
      "Epoch [99/300], Step [391/391], Loss: 0.1818, Accuracy: 93.5720\n",
      "Loss: 0.4924, Accuracy: 85.6400\n",
      "Epoch [100/300], Step [100/391], Loss: 0.1850, Accuracy: 93.1172\n",
      "Epoch [100/300], Step [200/391], Loss: 0.1826, Accuracy: 93.2969\n",
      "Epoch [100/300], Step [300/391], Loss: 0.1830, Accuracy: 93.3984\n",
      "Epoch [100/300], Step [391/391], Loss: 0.1860, Accuracy: 93.2560\n",
      "Loss: 0.5053, Accuracy: 85.8400\n",
      "Epoch [101/300], Step [100/391], Loss: 0.1799, Accuracy: 93.5547\n",
      "Epoch [101/300], Step [200/391], Loss: 0.1796, Accuracy: 93.5352\n",
      "Epoch [101/300], Step [300/391], Loss: 0.1769, Accuracy: 93.6927\n",
      "Epoch [101/300], Step [391/391], Loss: 0.1782, Accuracy: 93.5920\n",
      "Loss: 0.5040, Accuracy: 85.0900\n",
      "Epoch [102/300], Step [100/391], Loss: 0.1840, Accuracy: 93.2891\n",
      "Epoch [102/300], Step [200/391], Loss: 0.1807, Accuracy: 93.4727\n",
      "Epoch [102/300], Step [300/391], Loss: 0.1815, Accuracy: 93.5625\n",
      "Epoch [102/300], Step [391/391], Loss: 0.1837, Accuracy: 93.5320\n",
      "Loss: 0.5029, Accuracy: 85.3800\n",
      "Epoch [103/300], Step [100/391], Loss: 0.1783, Accuracy: 93.5312\n",
      "Epoch [103/300], Step [200/391], Loss: 0.1809, Accuracy: 93.4219\n",
      "Epoch [103/300], Step [300/391], Loss: 0.1816, Accuracy: 93.4714\n",
      "Epoch [103/300], Step [391/391], Loss: 0.1825, Accuracy: 93.4640\n",
      "Loss: 0.5008, Accuracy: 85.0000\n",
      "Epoch [104/300], Step [100/391], Loss: 0.1809, Accuracy: 93.6797\n",
      "Epoch [104/300], Step [200/391], Loss: 0.1814, Accuracy: 93.5039\n",
      "Epoch [104/300], Step [300/391], Loss: 0.1833, Accuracy: 93.4688\n",
      "Epoch [104/300], Step [391/391], Loss: 0.1828, Accuracy: 93.4500\n",
      "Loss: 0.5004, Accuracy: 85.5000\n",
      "Epoch [105/300], Step [100/391], Loss: 0.1784, Accuracy: 93.8281\n",
      "Epoch [105/300], Step [200/391], Loss: 0.1782, Accuracy: 93.6289\n",
      "Epoch [105/300], Step [300/391], Loss: 0.1809, Accuracy: 93.5625\n",
      "Epoch [105/300], Step [391/391], Loss: 0.1809, Accuracy: 93.5920\n",
      "Loss: 0.5105, Accuracy: 85.4800\n",
      "Epoch [106/300], Step [100/391], Loss: 0.1918, Accuracy: 93.1094\n",
      "Epoch [106/300], Step [200/391], Loss: 0.1861, Accuracy: 93.3086\n",
      "Epoch [106/300], Step [300/391], Loss: 0.1825, Accuracy: 93.5182\n",
      "Epoch [106/300], Step [391/391], Loss: 0.1824, Accuracy: 93.4360\n",
      "Loss: 0.5021, Accuracy: 85.4400\n",
      "Epoch [107/300], Step [100/391], Loss: 0.1877, Accuracy: 93.4062\n",
      "Epoch [107/300], Step [200/391], Loss: 0.1776, Accuracy: 93.6758\n",
      "Epoch [107/300], Step [300/391], Loss: 0.1794, Accuracy: 93.5859\n",
      "Epoch [107/300], Step [391/391], Loss: 0.1795, Accuracy: 93.6040\n",
      "Loss: 0.5106, Accuracy: 85.3800\n",
      "Epoch [108/300], Step [100/391], Loss: 0.1807, Accuracy: 93.6797\n",
      "Epoch [108/300], Step [200/391], Loss: 0.1788, Accuracy: 93.7461\n",
      "Epoch [108/300], Step [300/391], Loss: 0.1815, Accuracy: 93.6458\n",
      "Epoch [108/300], Step [391/391], Loss: 0.1814, Accuracy: 93.6240\n",
      "Loss: 0.5029, Accuracy: 85.4600\n",
      "Epoch [109/300], Step [100/391], Loss: 0.1814, Accuracy: 93.6172\n",
      "Epoch [109/300], Step [200/391], Loss: 0.1846, Accuracy: 93.3984\n",
      "Epoch [109/300], Step [300/391], Loss: 0.1801, Accuracy: 93.5469\n",
      "Epoch [109/300], Step [391/391], Loss: 0.1813, Accuracy: 93.4840\n",
      "Loss: 0.5106, Accuracy: 85.0300\n",
      "Epoch [110/300], Step [100/391], Loss: 0.1811, Accuracy: 93.3438\n",
      "Epoch [110/300], Step [200/391], Loss: 0.1874, Accuracy: 93.1133\n",
      "Epoch [110/300], Step [300/391], Loss: 0.1857, Accuracy: 93.2760\n",
      "Epoch [110/300], Step [391/391], Loss: 0.1845, Accuracy: 93.3960\n",
      "Loss: 0.5046, Accuracy: 85.5400\n",
      "Epoch [111/300], Step [100/391], Loss: 0.1761, Accuracy: 93.7188\n",
      "Epoch [111/300], Step [200/391], Loss: 0.1804, Accuracy: 93.5859\n",
      "Epoch [111/300], Step [300/391], Loss: 0.1795, Accuracy: 93.5599\n",
      "Epoch [111/300], Step [391/391], Loss: 0.1800, Accuracy: 93.5760\n",
      "Loss: 0.5093, Accuracy: 85.0200\n",
      "Epoch [112/300], Step [100/391], Loss: 0.1813, Accuracy: 93.4141\n",
      "Epoch [112/300], Step [200/391], Loss: 0.1833, Accuracy: 93.4023\n",
      "Epoch [112/300], Step [300/391], Loss: 0.1828, Accuracy: 93.4609\n",
      "Epoch [112/300], Step [391/391], Loss: 0.1816, Accuracy: 93.4940\n",
      "Loss: 0.5129, Accuracy: 85.0900\n",
      "Epoch [113/300], Step [100/391], Loss: 0.1819, Accuracy: 93.2422\n",
      "Epoch [113/300], Step [200/391], Loss: 0.1839, Accuracy: 93.4102\n",
      "Epoch [113/300], Step [300/391], Loss: 0.1841, Accuracy: 93.3724\n",
      "Epoch [113/300], Step [391/391], Loss: 0.1830, Accuracy: 93.4080\n",
      "Loss: 0.5097, Accuracy: 84.8700\n",
      "Epoch [114/300], Step [100/391], Loss: 0.1812, Accuracy: 93.4609\n",
      "Epoch [114/300], Step [200/391], Loss: 0.1805, Accuracy: 93.4531\n",
      "Epoch [114/300], Step [300/391], Loss: 0.1819, Accuracy: 93.4870\n",
      "Epoch [114/300], Step [391/391], Loss: 0.1813, Accuracy: 93.4540\n",
      "Loss: 0.5055, Accuracy: 85.4000\n",
      "Epoch [115/300], Step [100/391], Loss: 0.1707, Accuracy: 93.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [115/300], Step [200/391], Loss: 0.1736, Accuracy: 93.7852\n",
      "Epoch [115/300], Step [300/391], Loss: 0.1772, Accuracy: 93.6589\n",
      "Epoch [115/300], Step [391/391], Loss: 0.1793, Accuracy: 93.5520\n",
      "Loss: 0.5018, Accuracy: 85.1800\n",
      "Epoch [116/300], Step [100/391], Loss: 0.1789, Accuracy: 93.7266\n",
      "Epoch [116/300], Step [200/391], Loss: 0.1792, Accuracy: 93.6406\n",
      "Epoch [116/300], Step [300/391], Loss: 0.1812, Accuracy: 93.5052\n",
      "Epoch [116/300], Step [391/391], Loss: 0.1816, Accuracy: 93.5420\n",
      "Loss: 0.5075, Accuracy: 85.3600\n",
      "Epoch [117/300], Step [100/391], Loss: 0.1852, Accuracy: 93.3438\n",
      "Epoch [117/300], Step [200/391], Loss: 0.1797, Accuracy: 93.4570\n",
      "Epoch [117/300], Step [300/391], Loss: 0.1804, Accuracy: 93.6198\n",
      "Epoch [117/300], Step [391/391], Loss: 0.1810, Accuracy: 93.5080\n",
      "Loss: 0.5081, Accuracy: 85.1200\n",
      "Epoch [118/300], Step [100/391], Loss: 0.1861, Accuracy: 93.4609\n",
      "Epoch [118/300], Step [200/391], Loss: 0.1807, Accuracy: 93.6172\n",
      "Epoch [118/300], Step [300/391], Loss: 0.1801, Accuracy: 93.5885\n",
      "Epoch [118/300], Step [391/391], Loss: 0.1815, Accuracy: 93.5980\n",
      "Loss: 0.4941, Accuracy: 85.6300\n",
      "Epoch [119/300], Step [100/391], Loss: 0.1874, Accuracy: 93.4062\n",
      "Epoch [119/300], Step [200/391], Loss: 0.1880, Accuracy: 93.3711\n",
      "Epoch [119/300], Step [300/391], Loss: 0.1838, Accuracy: 93.3958\n",
      "Epoch [119/300], Step [391/391], Loss: 0.1818, Accuracy: 93.4980\n",
      "Loss: 0.4990, Accuracy: 85.3900\n",
      "Epoch [120/300], Step [100/391], Loss: 0.1825, Accuracy: 93.3125\n",
      "Epoch [120/300], Step [200/391], Loss: 0.1786, Accuracy: 93.5117\n",
      "Epoch [120/300], Step [300/391], Loss: 0.1784, Accuracy: 93.5130\n",
      "Epoch [120/300], Step [391/391], Loss: 0.1780, Accuracy: 93.5300\n",
      "Loss: 0.5037, Accuracy: 85.3800\n",
      "Epoch [121/300], Step [100/391], Loss: 0.1806, Accuracy: 93.7109\n",
      "Epoch [121/300], Step [200/391], Loss: 0.1828, Accuracy: 93.6016\n",
      "Epoch [121/300], Step [300/391], Loss: 0.1789, Accuracy: 93.7031\n",
      "Epoch [121/300], Step [391/391], Loss: 0.1786, Accuracy: 93.6660\n",
      "Loss: 0.5161, Accuracy: 85.1900\n",
      "Epoch [122/300], Step [100/391], Loss: 0.1794, Accuracy: 93.5000\n",
      "Epoch [122/300], Step [200/391], Loss: 0.1810, Accuracy: 93.5508\n",
      "Epoch [122/300], Step [300/391], Loss: 0.1812, Accuracy: 93.5469\n",
      "Epoch [122/300], Step [391/391], Loss: 0.1805, Accuracy: 93.5080\n",
      "Loss: 0.5021, Accuracy: 85.3800\n",
      "Epoch [123/300], Step [100/391], Loss: 0.1742, Accuracy: 93.7734\n",
      "Epoch [123/300], Step [200/391], Loss: 0.1773, Accuracy: 93.6797\n",
      "Epoch [123/300], Step [300/391], Loss: 0.1780, Accuracy: 93.6380\n",
      "Epoch [123/300], Step [391/391], Loss: 0.1793, Accuracy: 93.6220\n",
      "Loss: 0.5112, Accuracy: 85.1300\n",
      "Epoch [124/300], Step [100/391], Loss: 0.1730, Accuracy: 93.7891\n",
      "Epoch [124/300], Step [200/391], Loss: 0.1820, Accuracy: 93.5469\n",
      "Epoch [124/300], Step [300/391], Loss: 0.1827, Accuracy: 93.5000\n",
      "Epoch [124/300], Step [391/391], Loss: 0.1839, Accuracy: 93.4480\n",
      "Loss: 0.5011, Accuracy: 85.3500\n",
      "Epoch [125/300], Step [100/391], Loss: 0.1818, Accuracy: 93.4844\n",
      "Epoch [125/300], Step [200/391], Loss: 0.1782, Accuracy: 93.6406\n",
      "Epoch [125/300], Step [300/391], Loss: 0.1757, Accuracy: 93.7422\n",
      "Epoch [125/300], Step [391/391], Loss: 0.1767, Accuracy: 93.7080\n",
      "Loss: 0.5116, Accuracy: 85.3200\n",
      "Epoch [126/300], Step [100/391], Loss: 0.1861, Accuracy: 93.3516\n",
      "Epoch [126/300], Step [200/391], Loss: 0.1810, Accuracy: 93.4023\n",
      "Epoch [126/300], Step [300/391], Loss: 0.1790, Accuracy: 93.5521\n",
      "Epoch [126/300], Step [391/391], Loss: 0.1789, Accuracy: 93.5420\n",
      "Loss: 0.5070, Accuracy: 85.0800\n",
      "Epoch [127/300], Step [100/391], Loss: 0.1826, Accuracy: 93.3828\n",
      "Epoch [127/300], Step [200/391], Loss: 0.1825, Accuracy: 93.4727\n",
      "Epoch [127/300], Step [300/391], Loss: 0.1775, Accuracy: 93.6667\n",
      "Epoch [127/300], Step [391/391], Loss: 0.1793, Accuracy: 93.5400\n",
      "Loss: 0.4962, Accuracy: 85.5200\n",
      "Epoch [128/300], Step [100/391], Loss: 0.1763, Accuracy: 93.3828\n",
      "Epoch [128/300], Step [200/391], Loss: 0.1767, Accuracy: 93.5000\n",
      "Epoch [128/300], Step [300/391], Loss: 0.1763, Accuracy: 93.5833\n",
      "Epoch [128/300], Step [391/391], Loss: 0.1757, Accuracy: 93.6520\n",
      "Loss: 0.5067, Accuracy: 85.5600\n",
      "Epoch [129/300], Step [100/391], Loss: 0.1788, Accuracy: 93.5703\n",
      "Epoch [129/300], Step [200/391], Loss: 0.1766, Accuracy: 93.5781\n",
      "Epoch [129/300], Step [300/391], Loss: 0.1778, Accuracy: 93.5781\n",
      "Epoch [129/300], Step [391/391], Loss: 0.1766, Accuracy: 93.5840\n",
      "Loss: 0.5034, Accuracy: 85.7300\n",
      "Epoch [130/300], Step [100/391], Loss: 0.1757, Accuracy: 93.8125\n",
      "Epoch [130/300], Step [200/391], Loss: 0.1780, Accuracy: 93.7383\n",
      "Epoch [130/300], Step [300/391], Loss: 0.1754, Accuracy: 93.8307\n",
      "Epoch [130/300], Step [391/391], Loss: 0.1770, Accuracy: 93.7380\n",
      "Loss: 0.5105, Accuracy: 85.2000\n",
      "Epoch [131/300], Step [100/391], Loss: 0.1778, Accuracy: 93.6719\n",
      "Epoch [131/300], Step [200/391], Loss: 0.1822, Accuracy: 93.4609\n",
      "Epoch [131/300], Step [300/391], Loss: 0.1806, Accuracy: 93.4531\n",
      "Epoch [131/300], Step [391/391], Loss: 0.1803, Accuracy: 93.4600\n",
      "Loss: 0.4977, Accuracy: 85.4000\n",
      "Epoch [132/300], Step [100/391], Loss: 0.1761, Accuracy: 93.5703\n",
      "Epoch [132/300], Step [200/391], Loss: 0.1805, Accuracy: 93.5977\n",
      "Epoch [132/300], Step [300/391], Loss: 0.1794, Accuracy: 93.6406\n",
      "Epoch [132/300], Step [391/391], Loss: 0.1782, Accuracy: 93.6540\n",
      "Loss: 0.5047, Accuracy: 85.5500\n",
      "Epoch [133/300], Step [100/391], Loss: 0.1775, Accuracy: 93.3984\n",
      "Epoch [133/300], Step [200/391], Loss: 0.1831, Accuracy: 93.2656\n",
      "Epoch [133/300], Step [300/391], Loss: 0.1796, Accuracy: 93.4427\n",
      "Epoch [133/300], Step [391/391], Loss: 0.1788, Accuracy: 93.5400\n",
      "Loss: 0.5077, Accuracy: 85.4300\n",
      "Epoch [134/300], Step [100/391], Loss: 0.1778, Accuracy: 93.7188\n",
      "Epoch [134/300], Step [200/391], Loss: 0.1798, Accuracy: 93.6523\n",
      "Epoch [134/300], Step [300/391], Loss: 0.1773, Accuracy: 93.7292\n",
      "Epoch [134/300], Step [391/391], Loss: 0.1757, Accuracy: 93.7680\n",
      "Loss: 0.5067, Accuracy: 85.3200\n",
      "Epoch [135/300], Step [100/391], Loss: 0.1746, Accuracy: 93.9922\n",
      "Epoch [135/300], Step [200/391], Loss: 0.1787, Accuracy: 93.7148\n",
      "Epoch [135/300], Step [300/391], Loss: 0.1783, Accuracy: 93.7135\n",
      "Epoch [135/300], Step [391/391], Loss: 0.1783, Accuracy: 93.6700\n",
      "Loss: 0.5036, Accuracy: 85.5800\n",
      "Epoch [136/300], Step [100/391], Loss: 0.1770, Accuracy: 93.6328\n",
      "Epoch [136/300], Step [200/391], Loss: 0.1797, Accuracy: 93.3711\n",
      "Epoch [136/300], Step [300/391], Loss: 0.1811, Accuracy: 93.4531\n",
      "Epoch [136/300], Step [391/391], Loss: 0.1801, Accuracy: 93.5800\n",
      "Loss: 0.5163, Accuracy: 85.2300\n",
      "Epoch [137/300], Step [100/391], Loss: 0.1803, Accuracy: 93.5156\n",
      "Epoch [137/300], Step [200/391], Loss: 0.1826, Accuracy: 93.5039\n",
      "Epoch [137/300], Step [300/391], Loss: 0.1801, Accuracy: 93.6120\n",
      "Epoch [137/300], Step [391/391], Loss: 0.1799, Accuracy: 93.5880\n",
      "Loss: 0.5048, Accuracy: 85.2700\n",
      "Epoch [138/300], Step [100/391], Loss: 0.1768, Accuracy: 93.6641\n",
      "Epoch [138/300], Step [200/391], Loss: 0.1805, Accuracy: 93.5117\n",
      "Epoch [138/300], Step [300/391], Loss: 0.1798, Accuracy: 93.5443\n",
      "Epoch [138/300], Step [391/391], Loss: 0.1792, Accuracy: 93.5600\n",
      "Loss: 0.5057, Accuracy: 85.4200\n",
      "Epoch [139/300], Step [100/391], Loss: 0.1832, Accuracy: 93.3672\n",
      "Epoch [139/300], Step [200/391], Loss: 0.1824, Accuracy: 93.4492\n",
      "Epoch [139/300], Step [300/391], Loss: 0.1783, Accuracy: 93.5781\n",
      "Epoch [139/300], Step [391/391], Loss: 0.1786, Accuracy: 93.5760\n",
      "Loss: 0.4945, Accuracy: 85.7000\n",
      "Epoch [140/300], Step [100/391], Loss: 0.1714, Accuracy: 93.9219\n",
      "Epoch [140/300], Step [200/391], Loss: 0.1761, Accuracy: 93.7383\n",
      "Epoch [140/300], Step [300/391], Loss: 0.1774, Accuracy: 93.6432\n",
      "Epoch [140/300], Step [391/391], Loss: 0.1784, Accuracy: 93.6640\n",
      "Loss: 0.5044, Accuracy: 85.2000\n",
      "Epoch [141/300], Step [100/391], Loss: 0.1742, Accuracy: 93.7891\n",
      "Epoch [141/300], Step [200/391], Loss: 0.1796, Accuracy: 93.6445\n",
      "Epoch [141/300], Step [300/391], Loss: 0.1779, Accuracy: 93.7396\n",
      "Epoch [141/300], Step [391/391], Loss: 0.1781, Accuracy: 93.7240\n",
      "Loss: 0.4977, Accuracy: 85.7400\n",
      "Epoch [142/300], Step [100/391], Loss: 0.1745, Accuracy: 93.5391\n",
      "Epoch [142/300], Step [200/391], Loss: 0.1767, Accuracy: 93.5547\n",
      "Epoch [142/300], Step [300/391], Loss: 0.1784, Accuracy: 93.5391\n",
      "Epoch [142/300], Step [391/391], Loss: 0.1805, Accuracy: 93.4980\n",
      "Loss: 0.5034, Accuracy: 85.5200\n",
      "Epoch [143/300], Step [100/391], Loss: 0.1873, Accuracy: 93.1953\n",
      "Epoch [143/300], Step [200/391], Loss: 0.1804, Accuracy: 93.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [143/300], Step [300/391], Loss: 0.1786, Accuracy: 93.5651\n",
      "Epoch [143/300], Step [391/391], Loss: 0.1785, Accuracy: 93.6080\n",
      "Loss: 0.4950, Accuracy: 85.3900\n",
      "Epoch [144/300], Step [100/391], Loss: 0.1739, Accuracy: 93.6094\n",
      "Epoch [144/300], Step [200/391], Loss: 0.1754, Accuracy: 93.6133\n",
      "Epoch [144/300], Step [300/391], Loss: 0.1775, Accuracy: 93.5807\n",
      "Epoch [144/300], Step [391/391], Loss: 0.1778, Accuracy: 93.6020\n",
      "Loss: 0.4991, Accuracy: 85.6300\n",
      "Epoch [145/300], Step [100/391], Loss: 0.1833, Accuracy: 93.3438\n",
      "Epoch [145/300], Step [200/391], Loss: 0.1800, Accuracy: 93.4688\n",
      "Epoch [145/300], Step [300/391], Loss: 0.1778, Accuracy: 93.5677\n",
      "Epoch [145/300], Step [391/391], Loss: 0.1763, Accuracy: 93.6100\n",
      "Loss: 0.5080, Accuracy: 85.2600\n",
      "Epoch [146/300], Step [100/391], Loss: 0.1841, Accuracy: 93.5391\n",
      "Epoch [146/300], Step [200/391], Loss: 0.1796, Accuracy: 93.6484\n",
      "Epoch [146/300], Step [300/391], Loss: 0.1794, Accuracy: 93.6068\n",
      "Epoch [146/300], Step [391/391], Loss: 0.1790, Accuracy: 93.5940\n",
      "Loss: 0.4985, Accuracy: 85.5300\n",
      "Epoch [147/300], Step [100/391], Loss: 0.1861, Accuracy: 93.3594\n",
      "Epoch [147/300], Step [200/391], Loss: 0.1831, Accuracy: 93.4727\n",
      "Epoch [147/300], Step [300/391], Loss: 0.1817, Accuracy: 93.5365\n",
      "Epoch [147/300], Step [391/391], Loss: 0.1825, Accuracy: 93.4980\n",
      "Loss: 0.4962, Accuracy: 85.3100\n",
      "Epoch [148/300], Step [100/391], Loss: 0.1784, Accuracy: 93.5781\n",
      "Epoch [148/300], Step [200/391], Loss: 0.1774, Accuracy: 93.5859\n",
      "Epoch [148/300], Step [300/391], Loss: 0.1792, Accuracy: 93.5807\n",
      "Epoch [148/300], Step [391/391], Loss: 0.1802, Accuracy: 93.5860\n",
      "Loss: 0.5001, Accuracy: 85.4100\n",
      "Epoch [149/300], Step [100/391], Loss: 0.1758, Accuracy: 93.6094\n",
      "Epoch [149/300], Step [200/391], Loss: 0.1813, Accuracy: 93.5586\n",
      "Epoch [149/300], Step [300/391], Loss: 0.1820, Accuracy: 93.4974\n",
      "Epoch [149/300], Step [391/391], Loss: 0.1808, Accuracy: 93.5760\n",
      "Loss: 0.5112, Accuracy: 85.3800\n",
      "Epoch [150/300], Step [100/391], Loss: 0.1716, Accuracy: 93.7188\n",
      "Epoch [150/300], Step [200/391], Loss: 0.1760, Accuracy: 93.7969\n",
      "Epoch [150/300], Step [300/391], Loss: 0.1806, Accuracy: 93.6094\n",
      "Epoch [150/300], Step [391/391], Loss: 0.1795, Accuracy: 93.6360\n",
      "Loss: 0.5054, Accuracy: 85.1700\n",
      "Epoch [151/300], Step [100/391], Loss: 0.1772, Accuracy: 93.4844\n",
      "Epoch [151/300], Step [200/391], Loss: 0.1789, Accuracy: 93.5703\n",
      "Epoch [151/300], Step [300/391], Loss: 0.1791, Accuracy: 93.5964\n",
      "Epoch [151/300], Step [391/391], Loss: 0.1797, Accuracy: 93.5520\n",
      "Loss: 0.5148, Accuracy: 85.3200\n",
      "Epoch [152/300], Step [100/391], Loss: 0.1811, Accuracy: 93.4062\n",
      "Epoch [152/300], Step [200/391], Loss: 0.1790, Accuracy: 93.4766\n",
      "Epoch [152/300], Step [300/391], Loss: 0.1797, Accuracy: 93.5130\n",
      "Epoch [152/300], Step [391/391], Loss: 0.1786, Accuracy: 93.5360\n",
      "Loss: 0.4994, Accuracy: 85.4100\n",
      "Epoch [153/300], Step [100/391], Loss: 0.1820, Accuracy: 93.5391\n",
      "Epoch [153/300], Step [200/391], Loss: 0.1855, Accuracy: 93.6289\n",
      "Epoch [153/300], Step [300/391], Loss: 0.1815, Accuracy: 93.6328\n",
      "Epoch [153/300], Step [391/391], Loss: 0.1792, Accuracy: 93.7100\n",
      "Loss: 0.5104, Accuracy: 85.1700\n",
      "Epoch [154/300], Step [100/391], Loss: 0.1779, Accuracy: 93.7344\n",
      "Epoch [154/300], Step [200/391], Loss: 0.1779, Accuracy: 93.5742\n",
      "Epoch [154/300], Step [300/391], Loss: 0.1810, Accuracy: 93.4141\n",
      "Epoch [154/300], Step [391/391], Loss: 0.1787, Accuracy: 93.5660\n",
      "Loss: 0.4989, Accuracy: 85.6100\n",
      "Epoch [155/300], Step [100/391], Loss: 0.1710, Accuracy: 93.9531\n",
      "Epoch [155/300], Step [200/391], Loss: 0.1761, Accuracy: 93.8125\n",
      "Epoch [155/300], Step [300/391], Loss: 0.1780, Accuracy: 93.7656\n",
      "Epoch [155/300], Step [391/391], Loss: 0.1798, Accuracy: 93.6480\n",
      "Loss: 0.5132, Accuracy: 85.1700\n",
      "Epoch [156/300], Step [100/391], Loss: 0.1764, Accuracy: 93.6719\n",
      "Epoch [156/300], Step [200/391], Loss: 0.1789, Accuracy: 93.5820\n",
      "Epoch [156/300], Step [300/391], Loss: 0.1785, Accuracy: 93.6120\n",
      "Epoch [156/300], Step [391/391], Loss: 0.1788, Accuracy: 93.5560\n",
      "Loss: 0.5000, Accuracy: 85.6700\n",
      "Epoch [157/300], Step [100/391], Loss: 0.1909, Accuracy: 93.3438\n",
      "Epoch [157/300], Step [200/391], Loss: 0.1842, Accuracy: 93.4492\n",
      "Epoch [157/300], Step [300/391], Loss: 0.1825, Accuracy: 93.4766\n",
      "Epoch [157/300], Step [391/391], Loss: 0.1806, Accuracy: 93.5360\n",
      "Loss: 0.4916, Accuracy: 85.7500\n",
      "Epoch [158/300], Step [100/391], Loss: 0.1761, Accuracy: 93.6562\n",
      "Epoch [158/300], Step [200/391], Loss: 0.1843, Accuracy: 93.4531\n",
      "Epoch [158/300], Step [300/391], Loss: 0.1779, Accuracy: 93.6589\n",
      "Epoch [158/300], Step [391/391], Loss: 0.1770, Accuracy: 93.6660\n",
      "Loss: 0.5008, Accuracy: 85.2100\n",
      "Epoch [159/300], Step [100/391], Loss: 0.1823, Accuracy: 93.5391\n",
      "Epoch [159/300], Step [200/391], Loss: 0.1803, Accuracy: 93.5820\n",
      "Epoch [159/300], Step [300/391], Loss: 0.1777, Accuracy: 93.6536\n",
      "Epoch [159/300], Step [391/391], Loss: 0.1771, Accuracy: 93.6960\n",
      "Loss: 0.4903, Accuracy: 85.7100\n",
      "Epoch [160/300], Step [100/391], Loss: 0.1827, Accuracy: 93.3750\n",
      "Epoch [160/300], Step [200/391], Loss: 0.1791, Accuracy: 93.4023\n",
      "Epoch [160/300], Step [300/391], Loss: 0.1810, Accuracy: 93.4531\n",
      "Epoch [160/300], Step [391/391], Loss: 0.1791, Accuracy: 93.5420\n",
      "Loss: 0.5059, Accuracy: 85.6600\n",
      "Epoch [161/300], Step [100/391], Loss: 0.1800, Accuracy: 93.5625\n",
      "Epoch [161/300], Step [200/391], Loss: 0.1792, Accuracy: 93.4766\n",
      "Epoch [161/300], Step [300/391], Loss: 0.1788, Accuracy: 93.5677\n",
      "Epoch [161/300], Step [391/391], Loss: 0.1782, Accuracy: 93.6020\n",
      "Loss: 0.5073, Accuracy: 85.5800\n",
      "Epoch [162/300], Step [100/391], Loss: 0.1828, Accuracy: 93.5078\n",
      "Epoch [162/300], Step [200/391], Loss: 0.1817, Accuracy: 93.6094\n",
      "Epoch [162/300], Step [300/391], Loss: 0.1786, Accuracy: 93.5859\n",
      "Epoch [162/300], Step [391/391], Loss: 0.1776, Accuracy: 93.6400\n",
      "Loss: 0.4990, Accuracy: 85.4300\n",
      "Epoch [163/300], Step [100/391], Loss: 0.1727, Accuracy: 93.8281\n",
      "Epoch [163/300], Step [200/391], Loss: 0.1749, Accuracy: 93.7852\n",
      "Epoch [163/300], Step [300/391], Loss: 0.1783, Accuracy: 93.7083\n",
      "Epoch [163/300], Step [391/391], Loss: 0.1796, Accuracy: 93.6820\n",
      "Loss: 0.5020, Accuracy: 85.4300\n",
      "Epoch [164/300], Step [100/391], Loss: 0.1724, Accuracy: 94.0547\n",
      "Epoch [164/300], Step [200/391], Loss: 0.1760, Accuracy: 93.8633\n",
      "Epoch [164/300], Step [300/391], Loss: 0.1750, Accuracy: 93.7865\n",
      "Epoch [164/300], Step [391/391], Loss: 0.1755, Accuracy: 93.7460\n",
      "Loss: 0.5000, Accuracy: 85.6700\n",
      "Epoch [165/300], Step [100/391], Loss: 0.1699, Accuracy: 93.8984\n",
      "Epoch [165/300], Step [200/391], Loss: 0.1767, Accuracy: 93.6367\n",
      "Epoch [165/300], Step [300/391], Loss: 0.1767, Accuracy: 93.6510\n",
      "Epoch [165/300], Step [391/391], Loss: 0.1778, Accuracy: 93.5980\n",
      "Loss: 0.4968, Accuracy: 85.6800\n",
      "Epoch [166/300], Step [100/391], Loss: 0.1800, Accuracy: 93.4766\n",
      "Epoch [166/300], Step [200/391], Loss: 0.1809, Accuracy: 93.4844\n",
      "Epoch [166/300], Step [300/391], Loss: 0.1799, Accuracy: 93.5938\n",
      "Epoch [166/300], Step [391/391], Loss: 0.1773, Accuracy: 93.7140\n",
      "Loss: 0.5121, Accuracy: 85.4000\n",
      "Epoch [167/300], Step [100/391], Loss: 0.1874, Accuracy: 93.3359\n",
      "Epoch [167/300], Step [200/391], Loss: 0.1849, Accuracy: 93.4727\n",
      "Epoch [167/300], Step [300/391], Loss: 0.1820, Accuracy: 93.4974\n",
      "Epoch [167/300], Step [391/391], Loss: 0.1791, Accuracy: 93.6800\n",
      "Loss: 0.5103, Accuracy: 85.4600\n",
      "Epoch [168/300], Step [100/391], Loss: 0.1793, Accuracy: 93.6641\n",
      "Epoch [168/300], Step [200/391], Loss: 0.1777, Accuracy: 93.7461\n",
      "Epoch [168/300], Step [300/391], Loss: 0.1752, Accuracy: 93.7839\n",
      "Epoch [168/300], Step [391/391], Loss: 0.1785, Accuracy: 93.6940\n",
      "Loss: 0.4981, Accuracy: 85.4200\n",
      "Epoch [169/300], Step [100/391], Loss: 0.1794, Accuracy: 93.6562\n",
      "Epoch [169/300], Step [200/391], Loss: 0.1849, Accuracy: 93.4688\n",
      "Epoch [169/300], Step [300/391], Loss: 0.1824, Accuracy: 93.5573\n",
      "Epoch [169/300], Step [391/391], Loss: 0.1821, Accuracy: 93.5920\n",
      "Loss: 0.5059, Accuracy: 85.4400\n",
      "Epoch [170/300], Step [100/391], Loss: 0.1718, Accuracy: 93.8125\n",
      "Epoch [170/300], Step [200/391], Loss: 0.1736, Accuracy: 93.7930\n",
      "Epoch [170/300], Step [300/391], Loss: 0.1758, Accuracy: 93.6719\n",
      "Epoch [170/300], Step [391/391], Loss: 0.1787, Accuracy: 93.5400\n",
      "Loss: 0.5053, Accuracy: 85.3800\n",
      "Epoch [171/300], Step [100/391], Loss: 0.1706, Accuracy: 94.1406\n",
      "Epoch [171/300], Step [200/391], Loss: 0.1769, Accuracy: 93.7852\n",
      "Epoch [171/300], Step [300/391], Loss: 0.1776, Accuracy: 93.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/300], Step [391/391], Loss: 0.1800, Accuracy: 93.5900\n",
      "Loss: 0.5119, Accuracy: 85.4500\n",
      "Epoch [172/300], Step [100/391], Loss: 0.1790, Accuracy: 93.6562\n",
      "Epoch [172/300], Step [200/391], Loss: 0.1844, Accuracy: 93.4297\n",
      "Epoch [172/300], Step [300/391], Loss: 0.1807, Accuracy: 93.5573\n",
      "Epoch [172/300], Step [391/391], Loss: 0.1811, Accuracy: 93.5120\n",
      "Loss: 0.5150, Accuracy: 85.1600\n",
      "Epoch [173/300], Step [100/391], Loss: 0.1808, Accuracy: 93.2812\n",
      "Epoch [173/300], Step [200/391], Loss: 0.1795, Accuracy: 93.5234\n",
      "Epoch [173/300], Step [300/391], Loss: 0.1797, Accuracy: 93.5599\n",
      "Epoch [173/300], Step [391/391], Loss: 0.1812, Accuracy: 93.5160\n",
      "Loss: 0.4992, Accuracy: 85.4800\n",
      "Epoch [174/300], Step [100/391], Loss: 0.1805, Accuracy: 93.2656\n",
      "Epoch [174/300], Step [200/391], Loss: 0.1818, Accuracy: 93.4414\n",
      "Epoch [174/300], Step [300/391], Loss: 0.1810, Accuracy: 93.4870\n",
      "Epoch [174/300], Step [391/391], Loss: 0.1797, Accuracy: 93.5060\n",
      "Loss: 0.4975, Accuracy: 85.5300\n",
      "Epoch [175/300], Step [100/391], Loss: 0.1841, Accuracy: 93.3750\n",
      "Epoch [175/300], Step [200/391], Loss: 0.1802, Accuracy: 93.5273\n",
      "Epoch [175/300], Step [300/391], Loss: 0.1822, Accuracy: 93.4479\n",
      "Epoch [175/300], Step [391/391], Loss: 0.1794, Accuracy: 93.5060\n",
      "Loss: 0.5021, Accuracy: 85.7700\n",
      "Epoch [176/300], Step [100/391], Loss: 0.1805, Accuracy: 93.7031\n",
      "Epoch [176/300], Step [200/391], Loss: 0.1738, Accuracy: 93.8633\n",
      "Epoch [176/300], Step [300/391], Loss: 0.1780, Accuracy: 93.7135\n",
      "Epoch [176/300], Step [391/391], Loss: 0.1790, Accuracy: 93.6980\n",
      "Loss: 0.5120, Accuracy: 85.2400\n",
      "Epoch [177/300], Step [100/391], Loss: 0.1763, Accuracy: 93.6094\n",
      "Epoch [177/300], Step [200/391], Loss: 0.1791, Accuracy: 93.6328\n",
      "Epoch [177/300], Step [300/391], Loss: 0.1822, Accuracy: 93.5443\n",
      "Epoch [177/300], Step [391/391], Loss: 0.1815, Accuracy: 93.5800\n",
      "Loss: 0.5016, Accuracy: 85.4800\n",
      "Epoch [178/300], Step [100/391], Loss: 0.1775, Accuracy: 93.8672\n",
      "Epoch [178/300], Step [200/391], Loss: 0.1759, Accuracy: 93.8906\n",
      "Epoch [178/300], Step [300/391], Loss: 0.1775, Accuracy: 93.8125\n",
      "Epoch [178/300], Step [391/391], Loss: 0.1765, Accuracy: 93.8440\n",
      "Loss: 0.5140, Accuracy: 85.4400\n",
      "Epoch [179/300], Step [100/391], Loss: 0.1838, Accuracy: 93.1250\n",
      "Epoch [179/300], Step [200/391], Loss: 0.1807, Accuracy: 93.3711\n",
      "Epoch [179/300], Step [300/391], Loss: 0.1808, Accuracy: 93.4948\n",
      "Epoch [179/300], Step [391/391], Loss: 0.1814, Accuracy: 93.4960\n",
      "Loss: 0.5200, Accuracy: 84.7900\n",
      "Epoch [180/300], Step [100/391], Loss: 0.1782, Accuracy: 93.7344\n",
      "Epoch [180/300], Step [200/391], Loss: 0.1766, Accuracy: 93.8008\n",
      "Epoch [180/300], Step [300/391], Loss: 0.1770, Accuracy: 93.7891\n",
      "Epoch [180/300], Step [391/391], Loss: 0.1759, Accuracy: 93.7820\n",
      "Loss: 0.4932, Accuracy: 85.4500\n",
      "Epoch [181/300], Step [100/391], Loss: 0.1806, Accuracy: 93.5156\n",
      "Epoch [181/300], Step [200/391], Loss: 0.1784, Accuracy: 93.6562\n",
      "Epoch [181/300], Step [300/391], Loss: 0.1784, Accuracy: 93.6224\n",
      "Epoch [181/300], Step [391/391], Loss: 0.1796, Accuracy: 93.5960\n",
      "Loss: 0.5027, Accuracy: 85.4600\n",
      "Epoch [182/300], Step [100/391], Loss: 0.1753, Accuracy: 93.7656\n",
      "Epoch [182/300], Step [200/391], Loss: 0.1830, Accuracy: 93.4805\n",
      "Epoch [182/300], Step [300/391], Loss: 0.1809, Accuracy: 93.5781\n",
      "Epoch [182/300], Step [391/391], Loss: 0.1789, Accuracy: 93.6840\n",
      "Loss: 0.4944, Accuracy: 85.3000\n",
      "Epoch [183/300], Step [100/391], Loss: 0.1718, Accuracy: 93.7266\n",
      "Epoch [183/300], Step [200/391], Loss: 0.1747, Accuracy: 93.8164\n",
      "Epoch [183/300], Step [300/391], Loss: 0.1777, Accuracy: 93.6589\n",
      "Epoch [183/300], Step [391/391], Loss: 0.1755, Accuracy: 93.7660\n",
      "Loss: 0.5141, Accuracy: 85.2700\n",
      "Epoch [184/300], Step [100/391], Loss: 0.1721, Accuracy: 93.6719\n",
      "Epoch [184/300], Step [200/391], Loss: 0.1714, Accuracy: 93.7734\n",
      "Epoch [184/300], Step [300/391], Loss: 0.1728, Accuracy: 93.7812\n",
      "Epoch [184/300], Step [391/391], Loss: 0.1735, Accuracy: 93.8000\n",
      "Loss: 0.5048, Accuracy: 85.1700\n",
      "Epoch [185/300], Step [100/391], Loss: 0.1766, Accuracy: 93.7109\n",
      "Epoch [185/300], Step [200/391], Loss: 0.1781, Accuracy: 93.5781\n",
      "Epoch [185/300], Step [300/391], Loss: 0.1782, Accuracy: 93.5885\n",
      "Epoch [185/300], Step [391/391], Loss: 0.1792, Accuracy: 93.6240\n",
      "Loss: 0.4985, Accuracy: 85.2200\n",
      "Epoch [186/300], Step [100/391], Loss: 0.1714, Accuracy: 93.6016\n",
      "Epoch [186/300], Step [200/391], Loss: 0.1742, Accuracy: 93.7305\n",
      "Epoch [186/300], Step [300/391], Loss: 0.1742, Accuracy: 93.7578\n",
      "Epoch [186/300], Step [391/391], Loss: 0.1759, Accuracy: 93.6180\n",
      "Loss: 0.5040, Accuracy: 85.7000\n",
      "Epoch [187/300], Step [100/391], Loss: 0.1812, Accuracy: 93.5859\n",
      "Epoch [187/300], Step [200/391], Loss: 0.1777, Accuracy: 93.6719\n",
      "Epoch [187/300], Step [300/391], Loss: 0.1764, Accuracy: 93.6458\n",
      "Epoch [187/300], Step [391/391], Loss: 0.1801, Accuracy: 93.5240\n",
      "Loss: 0.5146, Accuracy: 85.4000\n",
      "Epoch [188/300], Step [100/391], Loss: 0.1757, Accuracy: 93.5234\n",
      "Epoch [188/300], Step [200/391], Loss: 0.1765, Accuracy: 93.5586\n",
      "Epoch [188/300], Step [300/391], Loss: 0.1778, Accuracy: 93.5677\n",
      "Epoch [188/300], Step [391/391], Loss: 0.1796, Accuracy: 93.4580\n",
      "Loss: 0.5095, Accuracy: 85.4400\n",
      "Epoch [189/300], Step [100/391], Loss: 0.1851, Accuracy: 93.4375\n",
      "Epoch [189/300], Step [200/391], Loss: 0.1771, Accuracy: 93.6406\n",
      "Epoch [189/300], Step [300/391], Loss: 0.1779, Accuracy: 93.6589\n",
      "Epoch [189/300], Step [391/391], Loss: 0.1773, Accuracy: 93.6360\n",
      "Loss: 0.5182, Accuracy: 85.0200\n",
      "Epoch [190/300], Step [100/391], Loss: 0.1749, Accuracy: 93.8125\n",
      "Epoch [190/300], Step [200/391], Loss: 0.1724, Accuracy: 93.8203\n",
      "Epoch [190/300], Step [300/391], Loss: 0.1741, Accuracy: 93.7682\n",
      "Epoch [190/300], Step [391/391], Loss: 0.1776, Accuracy: 93.6180\n",
      "Loss: 0.5074, Accuracy: 85.3800\n",
      "Epoch [191/300], Step [100/391], Loss: 0.1779, Accuracy: 93.3516\n",
      "Epoch [191/300], Step [200/391], Loss: 0.1778, Accuracy: 93.5078\n",
      "Epoch [191/300], Step [300/391], Loss: 0.1772, Accuracy: 93.5755\n",
      "Epoch [191/300], Step [391/391], Loss: 0.1788, Accuracy: 93.5400\n",
      "Loss: 0.5031, Accuracy: 85.5500\n",
      "Epoch [192/300], Step [100/391], Loss: 0.1760, Accuracy: 93.8906\n",
      "Epoch [192/300], Step [200/391], Loss: 0.1813, Accuracy: 93.5781\n",
      "Epoch [192/300], Step [300/391], Loss: 0.1768, Accuracy: 93.8151\n",
      "Epoch [192/300], Step [391/391], Loss: 0.1780, Accuracy: 93.7580\n",
      "Loss: 0.5018, Accuracy: 85.4300\n",
      "Epoch [193/300], Step [100/391], Loss: 0.1844, Accuracy: 93.2969\n",
      "Epoch [193/300], Step [200/391], Loss: 0.1795, Accuracy: 93.5078\n",
      "Epoch [193/300], Step [300/391], Loss: 0.1761, Accuracy: 93.5990\n",
      "Epoch [193/300], Step [391/391], Loss: 0.1776, Accuracy: 93.5780\n",
      "Loss: 0.5041, Accuracy: 85.3200\n",
      "Epoch [194/300], Step [100/391], Loss: 0.1903, Accuracy: 93.2188\n",
      "Epoch [194/300], Step [200/391], Loss: 0.1882, Accuracy: 93.3203\n",
      "Epoch [194/300], Step [300/391], Loss: 0.1800, Accuracy: 93.6276\n",
      "Epoch [194/300], Step [391/391], Loss: 0.1782, Accuracy: 93.7140\n",
      "Loss: 0.5096, Accuracy: 85.6500\n",
      "Epoch [195/300], Step [100/391], Loss: 0.1789, Accuracy: 93.7812\n",
      "Epoch [195/300], Step [200/391], Loss: 0.1775, Accuracy: 93.7383\n",
      "Epoch [195/300], Step [300/391], Loss: 0.1771, Accuracy: 93.6927\n",
      "Epoch [195/300], Step [391/391], Loss: 0.1767, Accuracy: 93.6840\n",
      "Loss: 0.5128, Accuracy: 85.3900\n",
      "Epoch [196/300], Step [100/391], Loss: 0.1811, Accuracy: 93.5078\n",
      "Epoch [196/300], Step [200/391], Loss: 0.1795, Accuracy: 93.6016\n",
      "Epoch [196/300], Step [300/391], Loss: 0.1785, Accuracy: 93.6797\n",
      "Epoch [196/300], Step [391/391], Loss: 0.1779, Accuracy: 93.6380\n",
      "Loss: 0.5075, Accuracy: 85.1300\n",
      "Epoch [197/300], Step [100/391], Loss: 0.1837, Accuracy: 93.3125\n",
      "Epoch [197/300], Step [200/391], Loss: 0.1776, Accuracy: 93.6328\n",
      "Epoch [197/300], Step [300/391], Loss: 0.1801, Accuracy: 93.4896\n",
      "Epoch [197/300], Step [391/391], Loss: 0.1787, Accuracy: 93.5680\n",
      "Loss: 0.5051, Accuracy: 85.2800\n",
      "Epoch [198/300], Step [100/391], Loss: 0.1818, Accuracy: 93.4688\n",
      "Epoch [198/300], Step [200/391], Loss: 0.1768, Accuracy: 93.6875\n",
      "Epoch [198/300], Step [300/391], Loss: 0.1783, Accuracy: 93.6979\n",
      "Epoch [198/300], Step [391/391], Loss: 0.1764, Accuracy: 93.7200\n",
      "Loss: 0.5071, Accuracy: 85.5000\n",
      "Epoch [199/300], Step [100/391], Loss: 0.1821, Accuracy: 93.5469\n",
      "Epoch [199/300], Step [200/391], Loss: 0.1802, Accuracy: 93.5781\n",
      "Epoch [199/300], Step [300/391], Loss: 0.1777, Accuracy: 93.6302\n",
      "Epoch [199/300], Step [391/391], Loss: 0.1776, Accuracy: 93.6040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5113, Accuracy: 85.5600\n",
      "Epoch [200/300], Step [100/391], Loss: 0.1665, Accuracy: 93.9688\n",
      "Epoch [200/300], Step [200/391], Loss: 0.1713, Accuracy: 93.8320\n",
      "Epoch [200/300], Step [300/391], Loss: 0.1777, Accuracy: 93.6406\n",
      "Epoch [200/300], Step [391/391], Loss: 0.1783, Accuracy: 93.6300\n",
      "Loss: 0.5043, Accuracy: 85.4900\n",
      "Epoch [201/300], Step [100/391], Loss: 0.1719, Accuracy: 93.6562\n",
      "Epoch [201/300], Step [200/391], Loss: 0.1782, Accuracy: 93.4336\n",
      "Epoch [201/300], Step [300/391], Loss: 0.1757, Accuracy: 93.6406\n",
      "Epoch [201/300], Step [391/391], Loss: 0.1752, Accuracy: 93.7080\n",
      "Loss: 0.5019, Accuracy: 85.3000\n",
      "Epoch [202/300], Step [100/391], Loss: 0.1785, Accuracy: 93.3594\n",
      "Epoch [202/300], Step [200/391], Loss: 0.1796, Accuracy: 93.5156\n",
      "Epoch [202/300], Step [300/391], Loss: 0.1785, Accuracy: 93.5729\n",
      "Epoch [202/300], Step [391/391], Loss: 0.1781, Accuracy: 93.5840\n",
      "Loss: 0.5013, Accuracy: 85.7200\n",
      "Epoch [203/300], Step [100/391], Loss: 0.1824, Accuracy: 93.5938\n",
      "Epoch [203/300], Step [200/391], Loss: 0.1799, Accuracy: 93.5625\n",
      "Epoch [203/300], Step [300/391], Loss: 0.1802, Accuracy: 93.5443\n",
      "Epoch [203/300], Step [391/391], Loss: 0.1792, Accuracy: 93.6120\n",
      "Loss: 0.5055, Accuracy: 85.2900\n",
      "Epoch [204/300], Step [100/391], Loss: 0.1824, Accuracy: 93.4609\n",
      "Epoch [204/300], Step [200/391], Loss: 0.1776, Accuracy: 93.6367\n",
      "Epoch [204/300], Step [300/391], Loss: 0.1769, Accuracy: 93.6745\n",
      "Epoch [204/300], Step [391/391], Loss: 0.1767, Accuracy: 93.7020\n",
      "Loss: 0.5150, Accuracy: 85.3700\n",
      "Epoch [205/300], Step [100/391], Loss: 0.1804, Accuracy: 93.6875\n",
      "Epoch [205/300], Step [200/391], Loss: 0.1824, Accuracy: 93.4688\n",
      "Epoch [205/300], Step [300/391], Loss: 0.1802, Accuracy: 93.5208\n",
      "Epoch [205/300], Step [391/391], Loss: 0.1789, Accuracy: 93.5340\n",
      "Loss: 0.5021, Accuracy: 85.5000\n",
      "Epoch [206/300], Step [100/391], Loss: 0.1795, Accuracy: 93.4297\n",
      "Epoch [206/300], Step [200/391], Loss: 0.1781, Accuracy: 93.4688\n",
      "Epoch [206/300], Step [300/391], Loss: 0.1767, Accuracy: 93.6250\n",
      "Epoch [206/300], Step [391/391], Loss: 0.1781, Accuracy: 93.5840\n",
      "Loss: 0.4956, Accuracy: 85.4200\n",
      "Epoch [207/300], Step [100/391], Loss: 0.1869, Accuracy: 93.3203\n",
      "Epoch [207/300], Step [200/391], Loss: 0.1791, Accuracy: 93.5352\n",
      "Epoch [207/300], Step [300/391], Loss: 0.1796, Accuracy: 93.4740\n",
      "Epoch [207/300], Step [391/391], Loss: 0.1793, Accuracy: 93.4700\n",
      "Loss: 0.4997, Accuracy: 85.3200\n",
      "Epoch [208/300], Step [100/391], Loss: 0.1711, Accuracy: 93.8125\n",
      "Epoch [208/300], Step [200/391], Loss: 0.1757, Accuracy: 93.7656\n",
      "Epoch [208/300], Step [300/391], Loss: 0.1754, Accuracy: 93.8438\n",
      "Epoch [208/300], Step [391/391], Loss: 0.1767, Accuracy: 93.7940\n",
      "Loss: 0.5023, Accuracy: 85.3900\n",
      "Epoch [209/300], Step [100/391], Loss: 0.1893, Accuracy: 93.3359\n",
      "Epoch [209/300], Step [200/391], Loss: 0.1814, Accuracy: 93.6211\n",
      "Epoch [209/300], Step [300/391], Loss: 0.1818, Accuracy: 93.6562\n",
      "Epoch [209/300], Step [391/391], Loss: 0.1793, Accuracy: 93.6980\n",
      "Loss: 0.4971, Accuracy: 85.5000\n",
      "Epoch [210/300], Step [100/391], Loss: 0.1704, Accuracy: 93.9844\n",
      "Epoch [210/300], Step [200/391], Loss: 0.1723, Accuracy: 93.8945\n",
      "Epoch [210/300], Step [300/391], Loss: 0.1755, Accuracy: 93.7422\n",
      "Epoch [210/300], Step [391/391], Loss: 0.1770, Accuracy: 93.6960\n",
      "Loss: 0.4955, Accuracy: 85.4300\n",
      "Epoch [211/300], Step [100/391], Loss: 0.1838, Accuracy: 93.5469\n",
      "Epoch [211/300], Step [200/391], Loss: 0.1798, Accuracy: 93.5664\n",
      "Epoch [211/300], Step [300/391], Loss: 0.1762, Accuracy: 93.7083\n",
      "Epoch [211/300], Step [391/391], Loss: 0.1779, Accuracy: 93.6300\n",
      "Loss: 0.5014, Accuracy: 85.5600\n",
      "Epoch [212/300], Step [100/391], Loss: 0.1784, Accuracy: 93.6016\n",
      "Epoch [212/300], Step [200/391], Loss: 0.1812, Accuracy: 93.4453\n",
      "Epoch [212/300], Step [300/391], Loss: 0.1798, Accuracy: 93.4531\n",
      "Epoch [212/300], Step [391/391], Loss: 0.1791, Accuracy: 93.5140\n",
      "Loss: 0.5119, Accuracy: 85.4700\n",
      "Epoch [213/300], Step [100/391], Loss: 0.1790, Accuracy: 93.3750\n",
      "Epoch [213/300], Step [200/391], Loss: 0.1834, Accuracy: 93.3633\n",
      "Epoch [213/300], Step [300/391], Loss: 0.1822, Accuracy: 93.4323\n",
      "Epoch [213/300], Step [391/391], Loss: 0.1799, Accuracy: 93.5360\n",
      "Loss: 0.5038, Accuracy: 85.1800\n",
      "Epoch [214/300], Step [100/391], Loss: 0.1841, Accuracy: 93.5078\n",
      "Epoch [214/300], Step [200/391], Loss: 0.1796, Accuracy: 93.5859\n",
      "Epoch [214/300], Step [300/391], Loss: 0.1772, Accuracy: 93.6797\n",
      "Epoch [214/300], Step [391/391], Loss: 0.1780, Accuracy: 93.6460\n",
      "Loss: 0.5111, Accuracy: 85.3500\n",
      "Epoch [215/300], Step [100/391], Loss: 0.1776, Accuracy: 93.6406\n",
      "Epoch [215/300], Step [200/391], Loss: 0.1775, Accuracy: 93.6172\n",
      "Epoch [215/300], Step [300/391], Loss: 0.1803, Accuracy: 93.5000\n",
      "Epoch [215/300], Step [391/391], Loss: 0.1783, Accuracy: 93.5980\n",
      "Loss: 0.5202, Accuracy: 85.4300\n",
      "Epoch [216/300], Step [100/391], Loss: 0.1727, Accuracy: 93.9219\n",
      "Epoch [216/300], Step [200/391], Loss: 0.1749, Accuracy: 93.8242\n",
      "Epoch [216/300], Step [300/391], Loss: 0.1761, Accuracy: 93.7604\n",
      "Epoch [216/300], Step [391/391], Loss: 0.1761, Accuracy: 93.7480\n",
      "Loss: 0.4939, Accuracy: 85.4700\n",
      "Epoch [217/300], Step [100/391], Loss: 0.1814, Accuracy: 93.4297\n",
      "Epoch [217/300], Step [200/391], Loss: 0.1846, Accuracy: 93.3281\n",
      "Epoch [217/300], Step [300/391], Loss: 0.1820, Accuracy: 93.4635\n",
      "Epoch [217/300], Step [391/391], Loss: 0.1801, Accuracy: 93.5780\n",
      "Loss: 0.4928, Accuracy: 85.5800\n",
      "Epoch [218/300], Step [100/391], Loss: 0.1775, Accuracy: 93.9141\n",
      "Epoch [218/300], Step [200/391], Loss: 0.1755, Accuracy: 93.8711\n",
      "Epoch [218/300], Step [300/391], Loss: 0.1773, Accuracy: 93.7188\n",
      "Epoch [218/300], Step [391/391], Loss: 0.1754, Accuracy: 93.7720\n",
      "Loss: 0.4987, Accuracy: 85.5800\n",
      "Epoch [219/300], Step [100/391], Loss: 0.1737, Accuracy: 93.7656\n",
      "Epoch [219/300], Step [200/391], Loss: 0.1737, Accuracy: 93.7461\n",
      "Epoch [219/300], Step [300/391], Loss: 0.1744, Accuracy: 93.7370\n",
      "Epoch [219/300], Step [391/391], Loss: 0.1765, Accuracy: 93.6240\n",
      "Loss: 0.5015, Accuracy: 85.2000\n",
      "Epoch [220/300], Step [100/391], Loss: 0.1755, Accuracy: 93.7812\n",
      "Epoch [220/300], Step [200/391], Loss: 0.1764, Accuracy: 93.6719\n",
      "Epoch [220/300], Step [300/391], Loss: 0.1769, Accuracy: 93.6302\n",
      "Epoch [220/300], Step [391/391], Loss: 0.1770, Accuracy: 93.5800\n",
      "Loss: 0.4956, Accuracy: 85.4300\n",
      "Epoch [221/300], Step [100/391], Loss: 0.1739, Accuracy: 93.8203\n",
      "Epoch [221/300], Step [200/391], Loss: 0.1731, Accuracy: 93.8086\n",
      "Epoch [221/300], Step [300/391], Loss: 0.1758, Accuracy: 93.8255\n",
      "Epoch [221/300], Step [391/391], Loss: 0.1760, Accuracy: 93.7460\n",
      "Loss: 0.5068, Accuracy: 85.7000\n",
      "Epoch [222/300], Step [100/391], Loss: 0.1855, Accuracy: 93.1641\n",
      "Epoch [222/300], Step [200/391], Loss: 0.1801, Accuracy: 93.4727\n",
      "Epoch [222/300], Step [300/391], Loss: 0.1779, Accuracy: 93.5859\n",
      "Epoch [222/300], Step [391/391], Loss: 0.1787, Accuracy: 93.4980\n",
      "Loss: 0.5041, Accuracy: 85.2900\n",
      "Epoch [223/300], Step [100/391], Loss: 0.1824, Accuracy: 93.4531\n",
      "Epoch [223/300], Step [200/391], Loss: 0.1818, Accuracy: 93.5820\n",
      "Epoch [223/300], Step [300/391], Loss: 0.1812, Accuracy: 93.6302\n",
      "Epoch [223/300], Step [391/391], Loss: 0.1805, Accuracy: 93.5860\n",
      "Loss: 0.5036, Accuracy: 85.5100\n",
      "Epoch [224/300], Step [100/391], Loss: 0.1815, Accuracy: 93.6094\n",
      "Epoch [224/300], Step [200/391], Loss: 0.1819, Accuracy: 93.4961\n",
      "Epoch [224/300], Step [300/391], Loss: 0.1804, Accuracy: 93.5495\n",
      "Epoch [224/300], Step [391/391], Loss: 0.1797, Accuracy: 93.6280\n",
      "Loss: 0.5274, Accuracy: 85.2200\n",
      "Epoch [225/300], Step [100/391], Loss: 0.1801, Accuracy: 93.5703\n",
      "Epoch [225/300], Step [200/391], Loss: 0.1832, Accuracy: 93.5117\n",
      "Epoch [225/300], Step [300/391], Loss: 0.1824, Accuracy: 93.5625\n",
      "Epoch [225/300], Step [391/391], Loss: 0.1792, Accuracy: 93.7120\n",
      "Loss: 0.5151, Accuracy: 85.2600\n",
      "Epoch [226/300], Step [100/391], Loss: 0.1758, Accuracy: 93.8672\n",
      "Epoch [226/300], Step [200/391], Loss: 0.1768, Accuracy: 93.7422\n",
      "Epoch [226/300], Step [300/391], Loss: 0.1782, Accuracy: 93.7292\n",
      "Epoch [226/300], Step [391/391], Loss: 0.1782, Accuracy: 93.7160\n",
      "Loss: 0.5030, Accuracy: 85.2700\n",
      "Epoch [227/300], Step [100/391], Loss: 0.1735, Accuracy: 93.6562\n",
      "Epoch [227/300], Step [200/391], Loss: 0.1763, Accuracy: 93.6562\n",
      "Epoch [227/300], Step [300/391], Loss: 0.1789, Accuracy: 93.6354\n",
      "Epoch [227/300], Step [391/391], Loss: 0.1789, Accuracy: 93.6040\n",
      "Loss: 0.5116, Accuracy: 85.2600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [228/300], Step [100/391], Loss: 0.1687, Accuracy: 93.8438\n",
      "Epoch [228/300], Step [200/391], Loss: 0.1735, Accuracy: 93.7461\n",
      "Epoch [228/300], Step [300/391], Loss: 0.1738, Accuracy: 93.7943\n",
      "Epoch [228/300], Step [391/391], Loss: 0.1760, Accuracy: 93.7000\n",
      "Loss: 0.5015, Accuracy: 85.7400\n",
      "Epoch [229/300], Step [100/391], Loss: 0.1793, Accuracy: 93.5000\n",
      "Epoch [229/300], Step [200/391], Loss: 0.1769, Accuracy: 93.6094\n",
      "Epoch [229/300], Step [300/391], Loss: 0.1767, Accuracy: 93.6068\n",
      "Epoch [229/300], Step [391/391], Loss: 0.1784, Accuracy: 93.5940\n",
      "Loss: 0.4977, Accuracy: 85.3500\n",
      "Epoch [230/300], Step [100/391], Loss: 0.1752, Accuracy: 93.7031\n",
      "Epoch [230/300], Step [200/391], Loss: 0.1752, Accuracy: 93.7617\n",
      "Epoch [230/300], Step [300/391], Loss: 0.1776, Accuracy: 93.6536\n",
      "Epoch [230/300], Step [391/391], Loss: 0.1785, Accuracy: 93.5920\n",
      "Loss: 0.5069, Accuracy: 85.2600\n",
      "Epoch [231/300], Step [100/391], Loss: 0.1727, Accuracy: 93.9375\n",
      "Epoch [231/300], Step [200/391], Loss: 0.1723, Accuracy: 93.9492\n",
      "Epoch [231/300], Step [300/391], Loss: 0.1754, Accuracy: 93.8047\n",
      "Epoch [231/300], Step [391/391], Loss: 0.1778, Accuracy: 93.6960\n",
      "Loss: 0.5158, Accuracy: 85.1300\n",
      "Epoch [232/300], Step [100/391], Loss: 0.1768, Accuracy: 93.6953\n",
      "Epoch [232/300], Step [200/391], Loss: 0.1729, Accuracy: 93.8438\n",
      "Epoch [232/300], Step [300/391], Loss: 0.1757, Accuracy: 93.7812\n",
      "Epoch [232/300], Step [391/391], Loss: 0.1781, Accuracy: 93.7500\n",
      "Loss: 0.5133, Accuracy: 85.3500\n",
      "Epoch [233/300], Step [100/391], Loss: 0.1833, Accuracy: 93.5312\n",
      "Epoch [233/300], Step [200/391], Loss: 0.1767, Accuracy: 93.7227\n",
      "Epoch [233/300], Step [300/391], Loss: 0.1769, Accuracy: 93.7734\n",
      "Epoch [233/300], Step [391/391], Loss: 0.1793, Accuracy: 93.6280\n",
      "Loss: 0.4957, Accuracy: 85.7200\n",
      "Epoch [234/300], Step [100/391], Loss: 0.1862, Accuracy: 93.5156\n",
      "Epoch [234/300], Step [200/391], Loss: 0.1845, Accuracy: 93.4727\n",
      "Epoch [234/300], Step [300/391], Loss: 0.1826, Accuracy: 93.5026\n",
      "Epoch [234/300], Step [391/391], Loss: 0.1796, Accuracy: 93.6140\n",
      "Loss: 0.5206, Accuracy: 85.5200\n",
      "Epoch [235/300], Step [100/391], Loss: 0.1723, Accuracy: 93.9453\n",
      "Epoch [235/300], Step [200/391], Loss: 0.1767, Accuracy: 93.9062\n",
      "Epoch [235/300], Step [300/391], Loss: 0.1770, Accuracy: 93.8958\n",
      "Epoch [235/300], Step [391/391], Loss: 0.1765, Accuracy: 93.9360\n",
      "Loss: 0.5018, Accuracy: 85.4100\n",
      "Epoch [236/300], Step [100/391], Loss: 0.1740, Accuracy: 93.8750\n",
      "Epoch [236/300], Step [200/391], Loss: 0.1758, Accuracy: 93.7695\n",
      "Epoch [236/300], Step [300/391], Loss: 0.1754, Accuracy: 93.7917\n",
      "Epoch [236/300], Step [391/391], Loss: 0.1767, Accuracy: 93.7940\n",
      "Loss: 0.5032, Accuracy: 85.5200\n",
      "Epoch [237/300], Step [100/391], Loss: 0.1778, Accuracy: 93.5156\n",
      "Epoch [237/300], Step [200/391], Loss: 0.1800, Accuracy: 93.5664\n",
      "Epoch [237/300], Step [300/391], Loss: 0.1797, Accuracy: 93.5625\n",
      "Epoch [237/300], Step [391/391], Loss: 0.1780, Accuracy: 93.5780\n",
      "Loss: 0.5091, Accuracy: 85.6200\n",
      "Epoch [238/300], Step [100/391], Loss: 0.1749, Accuracy: 93.7578\n",
      "Epoch [238/300], Step [200/391], Loss: 0.1734, Accuracy: 93.8086\n",
      "Epoch [238/300], Step [300/391], Loss: 0.1758, Accuracy: 93.7266\n",
      "Epoch [238/300], Step [391/391], Loss: 0.1750, Accuracy: 93.7720\n",
      "Loss: 0.4981, Accuracy: 85.6100\n",
      "Epoch [239/300], Step [100/391], Loss: 0.1761, Accuracy: 93.9688\n",
      "Epoch [239/300], Step [200/391], Loss: 0.1777, Accuracy: 93.7070\n",
      "Epoch [239/300], Step [300/391], Loss: 0.1789, Accuracy: 93.6068\n",
      "Epoch [239/300], Step [391/391], Loss: 0.1778, Accuracy: 93.6700\n",
      "Loss: 0.5191, Accuracy: 85.1700\n",
      "Epoch [240/300], Step [100/391], Loss: 0.1786, Accuracy: 93.3750\n",
      "Epoch [240/300], Step [200/391], Loss: 0.1798, Accuracy: 93.4609\n",
      "Epoch [240/300], Step [300/391], Loss: 0.1781, Accuracy: 93.6406\n",
      "Epoch [240/300], Step [391/391], Loss: 0.1802, Accuracy: 93.5900\n",
      "Loss: 0.4957, Accuracy: 85.4300\n",
      "Epoch [241/300], Step [100/391], Loss: 0.1769, Accuracy: 93.6484\n",
      "Epoch [241/300], Step [200/391], Loss: 0.1801, Accuracy: 93.5742\n",
      "Epoch [241/300], Step [300/391], Loss: 0.1773, Accuracy: 93.7656\n",
      "Epoch [241/300], Step [391/391], Loss: 0.1801, Accuracy: 93.6420\n",
      "Loss: 0.5003, Accuracy: 85.4000\n",
      "Epoch [242/300], Step [100/391], Loss: 0.1741, Accuracy: 93.5859\n",
      "Epoch [242/300], Step [200/391], Loss: 0.1731, Accuracy: 93.7344\n",
      "Epoch [242/300], Step [300/391], Loss: 0.1761, Accuracy: 93.6589\n",
      "Epoch [242/300], Step [391/391], Loss: 0.1772, Accuracy: 93.6440\n",
      "Loss: 0.5034, Accuracy: 85.5200\n",
      "Epoch [243/300], Step [100/391], Loss: 0.1752, Accuracy: 93.5781\n",
      "Epoch [243/300], Step [200/391], Loss: 0.1760, Accuracy: 93.6914\n",
      "Epoch [243/300], Step [300/391], Loss: 0.1778, Accuracy: 93.6667\n",
      "Epoch [243/300], Step [391/391], Loss: 0.1777, Accuracy: 93.6960\n",
      "Loss: 0.5099, Accuracy: 84.9600\n",
      "Epoch [244/300], Step [100/391], Loss: 0.1818, Accuracy: 93.7344\n",
      "Epoch [244/300], Step [200/391], Loss: 0.1812, Accuracy: 93.6250\n",
      "Epoch [244/300], Step [300/391], Loss: 0.1777, Accuracy: 93.5964\n",
      "Epoch [244/300], Step [391/391], Loss: 0.1783, Accuracy: 93.5920\n",
      "Loss: 0.5124, Accuracy: 85.3700\n",
      "Epoch [245/300], Step [100/391], Loss: 0.1719, Accuracy: 93.8828\n",
      "Epoch [245/300], Step [200/391], Loss: 0.1746, Accuracy: 93.7695\n",
      "Epoch [245/300], Step [300/391], Loss: 0.1763, Accuracy: 93.6823\n",
      "Epoch [245/300], Step [391/391], Loss: 0.1761, Accuracy: 93.6860\n",
      "Loss: 0.5054, Accuracy: 85.3600\n",
      "Epoch [246/300], Step [100/391], Loss: 0.1781, Accuracy: 93.4453\n",
      "Epoch [246/300], Step [200/391], Loss: 0.1766, Accuracy: 93.5898\n",
      "Epoch [246/300], Step [300/391], Loss: 0.1780, Accuracy: 93.5807\n",
      "Epoch [246/300], Step [391/391], Loss: 0.1779, Accuracy: 93.5660\n",
      "Loss: 0.5161, Accuracy: 85.2600\n",
      "Epoch [247/300], Step [100/391], Loss: 0.1846, Accuracy: 93.7500\n",
      "Epoch [247/300], Step [200/391], Loss: 0.1779, Accuracy: 93.8125\n",
      "Epoch [247/300], Step [300/391], Loss: 0.1815, Accuracy: 93.6562\n",
      "Epoch [247/300], Step [391/391], Loss: 0.1812, Accuracy: 93.6440\n",
      "Loss: 0.5002, Accuracy: 85.7400\n",
      "Epoch [248/300], Step [100/391], Loss: 0.1739, Accuracy: 93.9531\n",
      "Epoch [248/300], Step [200/391], Loss: 0.1735, Accuracy: 93.8984\n",
      "Epoch [248/300], Step [300/391], Loss: 0.1753, Accuracy: 93.8385\n",
      "Epoch [248/300], Step [391/391], Loss: 0.1763, Accuracy: 93.7260\n",
      "Loss: 0.5043, Accuracy: 85.4800\n",
      "Epoch [249/300], Step [100/391], Loss: 0.1860, Accuracy: 93.5000\n",
      "Epoch [249/300], Step [200/391], Loss: 0.1821, Accuracy: 93.6016\n",
      "Epoch [249/300], Step [300/391], Loss: 0.1799, Accuracy: 93.6562\n",
      "Epoch [249/300], Step [391/391], Loss: 0.1780, Accuracy: 93.6820\n",
      "Loss: 0.5014, Accuracy: 85.0500\n",
      "Epoch [250/300], Step [100/391], Loss: 0.1787, Accuracy: 93.5000\n",
      "Epoch [250/300], Step [200/391], Loss: 0.1776, Accuracy: 93.4609\n",
      "Epoch [250/300], Step [300/391], Loss: 0.1792, Accuracy: 93.5000\n",
      "Epoch [250/300], Step [391/391], Loss: 0.1781, Accuracy: 93.4980\n",
      "Loss: 0.4998, Accuracy: 85.6500\n",
      "Epoch [251/300], Step [100/391], Loss: 0.1801, Accuracy: 93.5000\n",
      "Epoch [251/300], Step [200/391], Loss: 0.1778, Accuracy: 93.6484\n",
      "Epoch [251/300], Step [300/391], Loss: 0.1793, Accuracy: 93.5964\n",
      "Epoch [251/300], Step [391/391], Loss: 0.1796, Accuracy: 93.5760\n",
      "Loss: 0.5110, Accuracy: 85.5100\n",
      "Epoch [252/300], Step [100/391], Loss: 0.1800, Accuracy: 93.7656\n",
      "Epoch [252/300], Step [200/391], Loss: 0.1803, Accuracy: 93.7773\n",
      "Epoch [252/300], Step [300/391], Loss: 0.1819, Accuracy: 93.6615\n",
      "Epoch [252/300], Step [391/391], Loss: 0.1806, Accuracy: 93.6680\n",
      "Loss: 0.5077, Accuracy: 85.2500\n",
      "Epoch [253/300], Step [100/391], Loss: 0.1725, Accuracy: 93.7969\n",
      "Epoch [253/300], Step [200/391], Loss: 0.1800, Accuracy: 93.6484\n",
      "Epoch [253/300], Step [300/391], Loss: 0.1804, Accuracy: 93.5938\n",
      "Epoch [253/300], Step [391/391], Loss: 0.1790, Accuracy: 93.6640\n",
      "Loss: 0.5175, Accuracy: 85.1100\n",
      "Epoch [254/300], Step [100/391], Loss: 0.1787, Accuracy: 93.5156\n",
      "Epoch [254/300], Step [200/391], Loss: 0.1750, Accuracy: 93.6836\n",
      "Epoch [254/300], Step [300/391], Loss: 0.1763, Accuracy: 93.7526\n",
      "Epoch [254/300], Step [391/391], Loss: 0.1775, Accuracy: 93.7020\n",
      "Loss: 0.5050, Accuracy: 85.2700\n",
      "Epoch [255/300], Step [100/391], Loss: 0.1881, Accuracy: 93.1406\n",
      "Epoch [255/300], Step [200/391], Loss: 0.1818, Accuracy: 93.3594\n",
      "Epoch [255/300], Step [300/391], Loss: 0.1807, Accuracy: 93.4271\n",
      "Epoch [255/300], Step [391/391], Loss: 0.1813, Accuracy: 93.4440\n",
      "Loss: 0.4949, Accuracy: 85.7800\n",
      "Epoch [256/300], Step [100/391], Loss: 0.1796, Accuracy: 93.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [256/300], Step [200/391], Loss: 0.1795, Accuracy: 93.5781\n",
      "Epoch [256/300], Step [300/391], Loss: 0.1806, Accuracy: 93.5417\n",
      "Epoch [256/300], Step [391/391], Loss: 0.1786, Accuracy: 93.6480\n",
      "Loss: 0.5158, Accuracy: 85.4400\n",
      "Epoch [257/300], Step [100/391], Loss: 0.1785, Accuracy: 93.7188\n",
      "Epoch [257/300], Step [200/391], Loss: 0.1808, Accuracy: 93.6133\n",
      "Epoch [257/300], Step [300/391], Loss: 0.1806, Accuracy: 93.5677\n",
      "Epoch [257/300], Step [391/391], Loss: 0.1804, Accuracy: 93.5700\n",
      "Loss: 0.5031, Accuracy: 85.2600\n",
      "Epoch [258/300], Step [100/391], Loss: 0.1756, Accuracy: 93.8906\n",
      "Epoch [258/300], Step [200/391], Loss: 0.1792, Accuracy: 93.6172\n",
      "Epoch [258/300], Step [300/391], Loss: 0.1769, Accuracy: 93.7135\n",
      "Epoch [258/300], Step [391/391], Loss: 0.1775, Accuracy: 93.6900\n",
      "Loss: 0.5060, Accuracy: 85.5600\n",
      "Epoch [259/300], Step [100/391], Loss: 0.1725, Accuracy: 93.9922\n",
      "Epoch [259/300], Step [200/391], Loss: 0.1766, Accuracy: 93.7344\n",
      "Epoch [259/300], Step [300/391], Loss: 0.1778, Accuracy: 93.6302\n",
      "Epoch [259/300], Step [391/391], Loss: 0.1795, Accuracy: 93.5540\n",
      "Loss: 0.5178, Accuracy: 85.1600\n",
      "Epoch [260/300], Step [100/391], Loss: 0.1771, Accuracy: 93.5625\n",
      "Epoch [260/300], Step [200/391], Loss: 0.1769, Accuracy: 93.6367\n",
      "Epoch [260/300], Step [300/391], Loss: 0.1774, Accuracy: 93.5807\n",
      "Epoch [260/300], Step [391/391], Loss: 0.1776, Accuracy: 93.5840\n",
      "Loss: 0.5038, Accuracy: 85.4800\n",
      "Epoch [261/300], Step [100/391], Loss: 0.1761, Accuracy: 93.7734\n",
      "Epoch [261/300], Step [200/391], Loss: 0.1754, Accuracy: 93.7227\n",
      "Epoch [261/300], Step [300/391], Loss: 0.1781, Accuracy: 93.5625\n",
      "Epoch [261/300], Step [391/391], Loss: 0.1775, Accuracy: 93.6380\n",
      "Loss: 0.5072, Accuracy: 85.4800\n",
      "Epoch [262/300], Step [100/391], Loss: 0.1720, Accuracy: 93.8359\n",
      "Epoch [262/300], Step [200/391], Loss: 0.1772, Accuracy: 93.6133\n",
      "Epoch [262/300], Step [300/391], Loss: 0.1749, Accuracy: 93.7318\n",
      "Epoch [262/300], Step [391/391], Loss: 0.1744, Accuracy: 93.7280\n",
      "Loss: 0.5123, Accuracy: 85.0600\n",
      "Epoch [263/300], Step [100/391], Loss: 0.1782, Accuracy: 93.5625\n",
      "Epoch [263/300], Step [200/391], Loss: 0.1818, Accuracy: 93.4570\n",
      "Epoch [263/300], Step [300/391], Loss: 0.1805, Accuracy: 93.5391\n",
      "Epoch [263/300], Step [391/391], Loss: 0.1797, Accuracy: 93.5580\n",
      "Loss: 0.4967, Accuracy: 85.7400\n",
      "Epoch [264/300], Step [100/391], Loss: 0.1841, Accuracy: 93.3438\n",
      "Epoch [264/300], Step [200/391], Loss: 0.1811, Accuracy: 93.4727\n",
      "Epoch [264/300], Step [300/391], Loss: 0.1781, Accuracy: 93.5781\n",
      "Epoch [264/300], Step [391/391], Loss: 0.1789, Accuracy: 93.5100\n",
      "Loss: 0.5109, Accuracy: 85.3300\n",
      "Epoch [265/300], Step [100/391], Loss: 0.1826, Accuracy: 93.3594\n",
      "Epoch [265/300], Step [200/391], Loss: 0.1815, Accuracy: 93.4609\n",
      "Epoch [265/300], Step [300/391], Loss: 0.1803, Accuracy: 93.4661\n",
      "Epoch [265/300], Step [391/391], Loss: 0.1803, Accuracy: 93.4960\n",
      "Loss: 0.5042, Accuracy: 85.3000\n",
      "Epoch [266/300], Step [100/391], Loss: 0.1815, Accuracy: 93.7344\n",
      "Epoch [266/300], Step [200/391], Loss: 0.1795, Accuracy: 93.6875\n",
      "Epoch [266/300], Step [300/391], Loss: 0.1787, Accuracy: 93.6380\n",
      "Epoch [266/300], Step [391/391], Loss: 0.1785, Accuracy: 93.6080\n",
      "Loss: 0.5082, Accuracy: 85.2000\n",
      "Epoch [267/300], Step [100/391], Loss: 0.1759, Accuracy: 93.6953\n",
      "Epoch [267/300], Step [200/391], Loss: 0.1758, Accuracy: 93.7109\n",
      "Epoch [267/300], Step [300/391], Loss: 0.1757, Accuracy: 93.6667\n",
      "Epoch [267/300], Step [391/391], Loss: 0.1765, Accuracy: 93.6580\n",
      "Loss: 0.5045, Accuracy: 85.4100\n",
      "Epoch [268/300], Step [100/391], Loss: 0.1786, Accuracy: 93.6328\n",
      "Epoch [268/300], Step [200/391], Loss: 0.1777, Accuracy: 93.6328\n",
      "Epoch [268/300], Step [300/391], Loss: 0.1784, Accuracy: 93.6432\n",
      "Epoch [268/300], Step [391/391], Loss: 0.1806, Accuracy: 93.6140\n",
      "Loss: 0.5134, Accuracy: 85.3800\n",
      "Epoch [269/300], Step [100/391], Loss: 0.1783, Accuracy: 93.5156\n",
      "Epoch [269/300], Step [200/391], Loss: 0.1822, Accuracy: 93.2891\n",
      "Epoch [269/300], Step [300/391], Loss: 0.1794, Accuracy: 93.4193\n",
      "Epoch [269/300], Step [391/391], Loss: 0.1781, Accuracy: 93.5200\n",
      "Loss: 0.4969, Accuracy: 85.3300\n",
      "Epoch [270/300], Step [100/391], Loss: 0.1713, Accuracy: 93.8828\n",
      "Epoch [270/300], Step [200/391], Loss: 0.1769, Accuracy: 93.7383\n",
      "Epoch [270/300], Step [300/391], Loss: 0.1782, Accuracy: 93.6641\n",
      "Epoch [270/300], Step [391/391], Loss: 0.1781, Accuracy: 93.6460\n",
      "Loss: 0.5064, Accuracy: 85.2700\n",
      "Epoch [271/300], Step [100/391], Loss: 0.1679, Accuracy: 93.9844\n",
      "Epoch [271/300], Step [200/391], Loss: 0.1740, Accuracy: 93.6875\n",
      "Epoch [271/300], Step [300/391], Loss: 0.1785, Accuracy: 93.5885\n",
      "Epoch [271/300], Step [391/391], Loss: 0.1801, Accuracy: 93.5540\n",
      "Loss: 0.4969, Accuracy: 85.7200\n",
      "Epoch [272/300], Step [100/391], Loss: 0.1752, Accuracy: 93.6641\n",
      "Epoch [272/300], Step [200/391], Loss: 0.1768, Accuracy: 93.5938\n",
      "Epoch [272/300], Step [300/391], Loss: 0.1783, Accuracy: 93.5781\n",
      "Epoch [272/300], Step [391/391], Loss: 0.1780, Accuracy: 93.5960\n",
      "Loss: 0.4980, Accuracy: 85.6800\n",
      "Epoch [273/300], Step [100/391], Loss: 0.1806, Accuracy: 93.5234\n",
      "Epoch [273/300], Step [200/391], Loss: 0.1806, Accuracy: 93.5781\n",
      "Epoch [273/300], Step [300/391], Loss: 0.1772, Accuracy: 93.7240\n",
      "Epoch [273/300], Step [391/391], Loss: 0.1784, Accuracy: 93.6760\n",
      "Loss: 0.5089, Accuracy: 84.6500\n",
      "Epoch [274/300], Step [100/391], Loss: 0.1760, Accuracy: 93.7500\n",
      "Epoch [274/300], Step [200/391], Loss: 0.1791, Accuracy: 93.5547\n",
      "Epoch [274/300], Step [300/391], Loss: 0.1820, Accuracy: 93.5182\n",
      "Epoch [274/300], Step [391/391], Loss: 0.1806, Accuracy: 93.5580\n",
      "Loss: 0.5214, Accuracy: 85.1800\n",
      "Epoch [275/300], Step [100/391], Loss: 0.1825, Accuracy: 93.7969\n",
      "Epoch [275/300], Step [200/391], Loss: 0.1801, Accuracy: 93.7188\n",
      "Epoch [275/300], Step [300/391], Loss: 0.1796, Accuracy: 93.6484\n",
      "Epoch [275/300], Step [391/391], Loss: 0.1801, Accuracy: 93.5900\n",
      "Loss: 0.5097, Accuracy: 85.4800\n",
      "Epoch [276/300], Step [100/391], Loss: 0.1758, Accuracy: 93.6094\n",
      "Epoch [276/300], Step [200/391], Loss: 0.1748, Accuracy: 93.7070\n",
      "Epoch [276/300], Step [300/391], Loss: 0.1769, Accuracy: 93.6380\n",
      "Epoch [276/300], Step [391/391], Loss: 0.1774, Accuracy: 93.5480\n",
      "Loss: 0.4966, Accuracy: 85.5200\n",
      "Epoch [277/300], Step [100/391], Loss: 0.1717, Accuracy: 93.8906\n",
      "Epoch [277/300], Step [200/391], Loss: 0.1775, Accuracy: 93.6914\n",
      "Epoch [277/300], Step [300/391], Loss: 0.1808, Accuracy: 93.5521\n",
      "Epoch [277/300], Step [391/391], Loss: 0.1819, Accuracy: 93.5420\n",
      "Loss: 0.5007, Accuracy: 85.5400\n",
      "Epoch [278/300], Step [100/391], Loss: 0.1798, Accuracy: 93.6094\n",
      "Epoch [278/300], Step [200/391], Loss: 0.1774, Accuracy: 93.6641\n",
      "Epoch [278/300], Step [300/391], Loss: 0.1788, Accuracy: 93.6380\n",
      "Epoch [278/300], Step [391/391], Loss: 0.1800, Accuracy: 93.5660\n",
      "Loss: 0.4978, Accuracy: 85.6700\n",
      "Epoch [279/300], Step [100/391], Loss: 0.1873, Accuracy: 93.2734\n",
      "Epoch [279/300], Step [200/391], Loss: 0.1802, Accuracy: 93.5820\n",
      "Epoch [279/300], Step [300/391], Loss: 0.1794, Accuracy: 93.6562\n",
      "Epoch [279/300], Step [391/391], Loss: 0.1783, Accuracy: 93.7200\n",
      "Loss: 0.5127, Accuracy: 85.2000\n",
      "Epoch [280/300], Step [100/391], Loss: 0.1775, Accuracy: 93.6797\n",
      "Epoch [280/300], Step [200/391], Loss: 0.1805, Accuracy: 93.6016\n",
      "Epoch [280/300], Step [300/391], Loss: 0.1805, Accuracy: 93.6302\n",
      "Epoch [280/300], Step [391/391], Loss: 0.1791, Accuracy: 93.6320\n",
      "Loss: 0.4995, Accuracy: 85.3900\n",
      "Epoch [281/300], Step [100/391], Loss: 0.1698, Accuracy: 93.9609\n",
      "Epoch [281/300], Step [200/391], Loss: 0.1779, Accuracy: 93.6953\n",
      "Epoch [281/300], Step [300/391], Loss: 0.1779, Accuracy: 93.6979\n",
      "Epoch [281/300], Step [391/391], Loss: 0.1788, Accuracy: 93.6880\n",
      "Loss: 0.4960, Accuracy: 85.6300\n",
      "Epoch [282/300], Step [100/391], Loss: 0.1738, Accuracy: 93.5547\n",
      "Epoch [282/300], Step [200/391], Loss: 0.1793, Accuracy: 93.5742\n",
      "Epoch [282/300], Step [300/391], Loss: 0.1792, Accuracy: 93.5417\n",
      "Epoch [282/300], Step [391/391], Loss: 0.1798, Accuracy: 93.5680\n",
      "Loss: 0.5131, Accuracy: 85.2700\n",
      "Epoch [283/300], Step [100/391], Loss: 0.1917, Accuracy: 93.2031\n",
      "Epoch [283/300], Step [200/391], Loss: 0.1816, Accuracy: 93.5703\n",
      "Epoch [283/300], Step [300/391], Loss: 0.1801, Accuracy: 93.5599\n",
      "Epoch [283/300], Step [391/391], Loss: 0.1790, Accuracy: 93.5940\n",
      "Loss: 0.5041, Accuracy: 85.6600\n",
      "Epoch [284/300], Step [100/391], Loss: 0.1749, Accuracy: 93.6797\n",
      "Epoch [284/300], Step [200/391], Loss: 0.1768, Accuracy: 93.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [284/300], Step [300/391], Loss: 0.1764, Accuracy: 93.7344\n",
      "Epoch [284/300], Step [391/391], Loss: 0.1790, Accuracy: 93.6620\n",
      "Loss: 0.5076, Accuracy: 85.1400\n",
      "Epoch [285/300], Step [100/391], Loss: 0.1830, Accuracy: 93.2891\n",
      "Epoch [285/300], Step [200/391], Loss: 0.1779, Accuracy: 93.6016\n",
      "Epoch [285/300], Step [300/391], Loss: 0.1791, Accuracy: 93.5469\n",
      "Epoch [285/300], Step [391/391], Loss: 0.1804, Accuracy: 93.4860\n",
      "Loss: 0.5076, Accuracy: 85.5000\n",
      "Epoch [286/300], Step [100/391], Loss: 0.1763, Accuracy: 93.6484\n",
      "Epoch [286/300], Step [200/391], Loss: 0.1772, Accuracy: 93.6719\n",
      "Epoch [286/300], Step [300/391], Loss: 0.1782, Accuracy: 93.6094\n",
      "Epoch [286/300], Step [391/391], Loss: 0.1794, Accuracy: 93.5980\n",
      "Loss: 0.5120, Accuracy: 85.2400\n",
      "Epoch [287/300], Step [100/391], Loss: 0.1805, Accuracy: 93.7734\n",
      "Epoch [287/300], Step [200/391], Loss: 0.1818, Accuracy: 93.5469\n",
      "Epoch [287/300], Step [300/391], Loss: 0.1803, Accuracy: 93.5495\n",
      "Epoch [287/300], Step [391/391], Loss: 0.1814, Accuracy: 93.5440\n",
      "Loss: 0.5182, Accuracy: 85.3800\n",
      "Epoch [288/300], Step [100/391], Loss: 0.1831, Accuracy: 93.4766\n",
      "Epoch [288/300], Step [200/391], Loss: 0.1825, Accuracy: 93.4180\n",
      "Epoch [288/300], Step [300/391], Loss: 0.1765, Accuracy: 93.6797\n",
      "Epoch [288/300], Step [391/391], Loss: 0.1781, Accuracy: 93.6420\n",
      "Loss: 0.5195, Accuracy: 85.3100\n",
      "Epoch [289/300], Step [100/391], Loss: 0.1798, Accuracy: 93.6016\n",
      "Epoch [289/300], Step [200/391], Loss: 0.1787, Accuracy: 93.5664\n",
      "Epoch [289/300], Step [300/391], Loss: 0.1760, Accuracy: 93.7370\n",
      "Epoch [289/300], Step [391/391], Loss: 0.1762, Accuracy: 93.7620\n",
      "Loss: 0.4901, Accuracy: 85.8500\n",
      "Epoch [290/300], Step [100/391], Loss: 0.1753, Accuracy: 93.6016\n",
      "Epoch [290/300], Step [200/391], Loss: 0.1765, Accuracy: 93.6055\n",
      "Epoch [290/300], Step [300/391], Loss: 0.1778, Accuracy: 93.5339\n",
      "Epoch [290/300], Step [391/391], Loss: 0.1784, Accuracy: 93.5440\n",
      "Loss: 0.4975, Accuracy: 85.4000\n",
      "Epoch [291/300], Step [100/391], Loss: 0.1827, Accuracy: 93.4922\n",
      "Epoch [291/300], Step [200/391], Loss: 0.1810, Accuracy: 93.6523\n",
      "Epoch [291/300], Step [300/391], Loss: 0.1794, Accuracy: 93.6797\n",
      "Epoch [291/300], Step [391/391], Loss: 0.1782, Accuracy: 93.6620\n",
      "Loss: 0.4963, Accuracy: 85.8200\n",
      "Epoch [292/300], Step [100/391], Loss: 0.1829, Accuracy: 93.5156\n",
      "Epoch [292/300], Step [200/391], Loss: 0.1823, Accuracy: 93.4922\n",
      "Epoch [292/300], Step [300/391], Loss: 0.1807, Accuracy: 93.5078\n",
      "Epoch [292/300], Step [391/391], Loss: 0.1783, Accuracy: 93.6380\n",
      "Loss: 0.5024, Accuracy: 85.1500\n",
      "Epoch [293/300], Step [100/391], Loss: 0.1739, Accuracy: 93.5859\n",
      "Epoch [293/300], Step [200/391], Loss: 0.1775, Accuracy: 93.4609\n",
      "Epoch [293/300], Step [300/391], Loss: 0.1781, Accuracy: 93.5859\n",
      "Epoch [293/300], Step [391/391], Loss: 0.1796, Accuracy: 93.5640\n",
      "Loss: 0.4926, Accuracy: 85.4200\n",
      "Epoch [294/300], Step [100/391], Loss: 0.1807, Accuracy: 93.5547\n",
      "Epoch [294/300], Step [200/391], Loss: 0.1857, Accuracy: 93.3984\n",
      "Epoch [294/300], Step [300/391], Loss: 0.1812, Accuracy: 93.5208\n",
      "Epoch [294/300], Step [391/391], Loss: 0.1783, Accuracy: 93.6380\n",
      "Loss: 0.5021, Accuracy: 85.5600\n",
      "Epoch [295/300], Step [100/391], Loss: 0.1810, Accuracy: 93.4766\n",
      "Epoch [295/300], Step [200/391], Loss: 0.1794, Accuracy: 93.6133\n",
      "Epoch [295/300], Step [300/391], Loss: 0.1792, Accuracy: 93.6120\n",
      "Epoch [295/300], Step [391/391], Loss: 0.1789, Accuracy: 93.5900\n",
      "Loss: 0.5100, Accuracy: 85.2100\n",
      "Epoch [296/300], Step [100/391], Loss: 0.1795, Accuracy: 93.6094\n",
      "Epoch [296/300], Step [200/391], Loss: 0.1794, Accuracy: 93.6836\n",
      "Epoch [296/300], Step [300/391], Loss: 0.1793, Accuracy: 93.6302\n",
      "Epoch [296/300], Step [391/391], Loss: 0.1778, Accuracy: 93.6860\n",
      "Loss: 0.5023, Accuracy: 85.4500\n",
      "Epoch [297/300], Step [100/391], Loss: 0.1767, Accuracy: 93.7500\n",
      "Epoch [297/300], Step [200/391], Loss: 0.1765, Accuracy: 93.7500\n",
      "Epoch [297/300], Step [300/391], Loss: 0.1764, Accuracy: 93.6979\n",
      "Epoch [297/300], Step [391/391], Loss: 0.1765, Accuracy: 93.6500\n",
      "Loss: 0.4975, Accuracy: 85.5800\n",
      "Epoch [298/300], Step [100/391], Loss: 0.1847, Accuracy: 93.3750\n",
      "Epoch [298/300], Step [200/391], Loss: 0.1765, Accuracy: 93.6641\n",
      "Epoch [298/300], Step [300/391], Loss: 0.1763, Accuracy: 93.7057\n",
      "Epoch [298/300], Step [391/391], Loss: 0.1784, Accuracy: 93.5860\n",
      "Loss: 0.5115, Accuracy: 85.2900\n",
      "Epoch [299/300], Step [100/391], Loss: 0.1748, Accuracy: 93.6875\n",
      "Epoch [299/300], Step [200/391], Loss: 0.1799, Accuracy: 93.6094\n",
      "Epoch [299/300], Step [300/391], Loss: 0.1795, Accuracy: 93.5026\n",
      "Epoch [299/300], Step [391/391], Loss: 0.1788, Accuracy: 93.5460\n",
      "Loss: 0.5016, Accuracy: 85.5200\n",
      "Epoch [300/300], Step [100/391], Loss: 0.1793, Accuracy: 93.7344\n",
      "Epoch [300/300], Step [200/391], Loss: 0.1778, Accuracy: 93.7383\n",
      "Epoch [300/300], Step [300/391], Loss: 0.1747, Accuracy: 93.8047\n",
      "Epoch [300/300], Step [391/391], Loss: 0.1763, Accuracy: 93.7920\n",
      "Loss: 0.5129, Accuracy: 85.5700\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "for epoch in range(num_epochs):\n",
    "    logs = open(logs_file,'a')\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    logs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934a7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "if color:\n",
    "    torch.save(model.state_dict(), f'{ResNet18_color}resnet_color.ckpt')\n",
    "else:\n",
    "    torch.save(model.state_dict(), f'{ResNet18_gray}resnet_gray.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7d244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
